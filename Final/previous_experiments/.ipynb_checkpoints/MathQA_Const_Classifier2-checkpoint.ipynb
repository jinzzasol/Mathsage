{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb351cc0",
   "metadata": {},
   "source": [
    "# MathQA Constant Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663cf3d7",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70b11732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585821d",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c5b29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_MODE = True # Set this to false to switch this into load and evaluate mode\n",
    "DATA_PATH = './dataset/'\n",
    "SET_NAMES = ['train', 'validation', 'test']\n",
    "MODEL_TYPE = 'microsoft/deberta-v3-large' # A more optimized version of roberta obtaining 95% of its performance\n",
    "MODEL_PATH = f'{MODEL_TYPE.split(\"/\")[-1]}-op_classifier-mathqa'\n",
    "MAX_LENGTH = 392\n",
    "NUM_MASK = '<num>'\n",
    "WORKING_DIR = 'TEMP/'\n",
    "FINAL_DIR = 'pickle/'\n",
    "\n",
    "class Op(Enum):\n",
    "    ADD = '+'\n",
    "    SUB = '-'\n",
    "    MULT = '*'\n",
    "    DIV = '/'\n",
    "    POW = '^'\n",
    "    \n",
    "class Const(Enum):\n",
    "    CONST_NEG_1 = 'const_neg_1' # I added this\n",
    "    CONST_0_25 = 'const_0_25'\n",
    "    CONST_0_2778 = 'const_0_2778'\n",
    "    CONST_0_33 = 'const_0_33'\n",
    "    CONST_0_3937 = 'const_0_3937'\n",
    "    CONST_1 = 'const_1'\n",
    "    CONST_1_6 = 'const_1_6'\n",
    "    CONST_2 = 'const_2'\n",
    "    CONST_3 = 'const_3'\n",
    "    CONST_PI = 'const_pi'\n",
    "    CONST_3_6 = 'const_3_6'\n",
    "    CONST_4 = 'const_4'\n",
    "    CONST_5 = 'const_5'\n",
    "    CONST_6 = 'const_6'\n",
    "    CONST_10 = 'const_10'\n",
    "    CONST_12 = 'const_12'\n",
    "    CONST_26 = 'const_26'\n",
    "    CONST_52 = 'const_52'\n",
    "    CONST_60 = 'const_60'\n",
    "    CONST_100 = 'const_100'\n",
    "    CONST_180 = 'const_180'\n",
    "    CONST_360 = 'const_360'\n",
    "    CONST_1000 = 'const_1000'\n",
    "    CONST_3600 = 'const_3600'\n",
    "\n",
    "values = [-1, 0.25, 0.2778, 0.33, 0.3937, 1, 1.6, 2, 3, math.pi, 3.6, 4, 5, 6, 10, 12, 26, 52, 60, 100, 180, 360, 1000, 3600]\n",
    "const2val = {k:v for k,v in zip(Const._value2member_map_.keys(), values)}    \n",
    "\n",
    "op2id = {k:v for k,v in zip(Op._value2member_map_.keys(), range(len(Op._value2member_map_)))}\n",
    "const2id = {k:v for k,v in zip(Const._value2member_map_.keys(), range(len(Const._value2member_map_)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f72619",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72bf445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {name:pd.read_csv(f'{DATA_PATH}{name}.csv') for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd0a06",
   "metadata": {},
   "source": [
    "This function converts all of the constants used in a problem to a single one hot encoded vector used for multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a65a490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_const(data):\n",
    "    labels = []\n",
    "    for num_set in data.nums:\n",
    "        num_set = eval(num_set)\n",
    "        idx = [const2id[num] for num in num_set if num in const2id]\n",
    "        onehot = np.zeros(len(const2id))\n",
    "        onehot[idx] = 1\n",
    "        labels.append(onehot)\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05dcf63",
   "metadata": {},
   "source": [
    "Getting only the necessary columns from the data (text/label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff23af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {name:Dataset.from_dict({'text':data[name]['problem'], 'labels':onehot_const(data[name])}) for name in SET_NAMES}\n",
    "data['train'].set_format('torch')\n",
    "data['validation'].set_format('torch')\n",
    "data['test'].set_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34c4ed",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a32b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "D:\\anaconda\\envs\\torch\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "def tokenization(items):\n",
    "    return tokenizer(items['text'], padding='max_length', max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "encoded = {k:v.map(tokenization, batched=True, remove_columns=['text']) for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d143dfdd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491fad7e",
   "metadata": {},
   "source": [
    "Training a model to predict which constants are used given a problem description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70974d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd1e6851c73497aa4ce8aabbc99294d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch = 4\n",
    "grad_acc = 4\n",
    "model_path = MODEL_PATH\n",
    "if TRAINING_MODE:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_TYPE, \n",
    "                                                               problem_type = 'multi_label_classification', \n",
    "                                                               num_labels = len(const2id))\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                               problem_type = 'multi_label_classification', \n",
    "                                                               num_labels = len(const2id))\n",
    "    \n",
    "def get_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'micro-f1': f1_score(y_true=y_true, y_pred=y_pred, average='micro'), \n",
    "        'macro-f1': f1_score(y_true=y_true, y_pred=y_pred, average='macro'),\n",
    "        'weighted-f1': f1_score(y_true=y_true, y_pred=y_pred, average='weighted'),\n",
    "        'accuracy': accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    }\n",
    "\n",
    "def compute_metrics_helper(p, label, thresh=0.5):\n",
    "    # Converting all values in the vector to be between 0 and 1\n",
    "    # The reason why softmax is not used is because we are doing multi label classification, meaning the total sum may be above 1\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    prob = sigmoid(torch.Tensor(p))\n",
    "    \n",
    "    # Converting items above the threshold to integers\n",
    "    y_pred = np.zeros(prob.shape)\n",
    "    y_pred[np.where(prob >= thresh)] = 1\n",
    "    \n",
    "    # Computing the metrics (f1, accuracy)\n",
    "    return get_metrics(label, y_pred)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return compute_metrics_helper(p.predictions, p.label_ids)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = f'{WORKING_DIR}{model_path}',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    learning_rate = 5e-6, # DeBERTa requires a lower learning rate\n",
    "    per_device_train_batch_size = batch,\n",
    "    per_device_eval_batch_size = batch,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    weight_decay=.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs = 5,\n",
    "    metric_for_best_model=\"accuracy\",  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = encoded['train'],\n",
    "    eval_dataset = encoded['validation'],\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8b10872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TRAINING_MODE:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        trainer.train()\n",
    "        trainer.save_model(model_path)\n",
    "else:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        predictions = {name:trainer.predict(encoded[name]) for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5718799",
   "metadata": {},
   "source": [
    "## Analysis and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e89336",
   "metadata": {},
   "source": [
    "As we can see, f1 and accuracy scores lie within the 80-85% accuracy range (except for macro f1, but we do not care about that as much for this specific problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a46595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \n",
      "{'micro-f1': 0.9210672595886603, 'macro-f1': 0.49350996493174204, 'weighted-f1': 0.9136586570272305, 'accuracy': 0.8795498215756244}\n",
      "\n",
      "VALIDATION: \n",
      "{'micro-f1': 0.8561643835616438, 'macro-f1': 0.4654305745971823, 'weighted-f1': 0.8468692698818778, 'accuracy': 0.8129151291512915}\n",
      "\n",
      "TEST: \n",
      "{'micro-f1': 0.8450920245398772, 'macro-f1': 0.4386897976899995, 'weighted-f1': 0.83637342170732, 'accuracy': 0.7975528364849833}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not TRAINING_MODE:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        for name in SET_NAMES:\n",
    "            print(f\"{name.upper()}: \")\n",
    "            print(compute_metrics_helper(predictions[name].predictions, predictions[name].label_ids))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef665b65",
   "metadata": {},
   "source": [
    "This is an acceptable accuracy, however, ideally this should be higher, especially if this is intended to be used on a downstream task. For our specific task, simply finding the constants that are useful for the problem, we care much more about recall than precision. Below we show some analysis by looking at the percentage of problems with missed constants and experimenting with the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b35bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_without_missed_const(p, label, thresh=0.5):\n",
    "    # Converting all values in the vector to be between 0 and 1\n",
    "    # The reason why softmax is not used is because we are doing multi label classification, meaning the total sum may be above 1\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    prob = sigmoid(torch.Tensor(p))\n",
    "    \n",
    "    # Converting items above the threshold to integers\n",
    "    y_pred = np.zeros(prob.shape)\n",
    "    y_pred[np.where(prob >= thresh)] = 1\n",
    "    \n",
    "    y_true = label\n",
    "    \n",
    "    # if the true set is a subset of the predicted set then it did not miss any constants in the problem\n",
    "    # returns avg # constants per problem, percent without any missed\n",
    "    return np.mean(np.sum(y_pred, axis=1)), np.sum([set(np.where(true==1)[0]) <= set(np.where(pred==1)[0]) for true, pred in zip(y_true, y_pred)])/len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d5922",
   "metadata": {},
   "source": [
    "Shown below is the percentage of problems without any missed constants along with the average number of constants predicted given different thresholds. Using a threshold of .005 on the validation set still results in over a 98% coverage, but reduces the average number of constants from 24 to just under 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e728cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THRESH: 0.5\n",
      "Percent without any missed: 0.8557195571955719\n",
      "Average number of constants per problem: 1.033579335793358\n",
      "\n",
      "THRESH: 0.05\n",
      "Percent without any missed: 0.9254612546125461\n",
      "Average number of constants per problem: 1.8154981549815499\n",
      "\n",
      "THRESH: 0.005\n",
      "Percent without any missed: 0.9826568265682657\n",
      "Average number of constants per problem: 5.815129151291513\n",
      "\n",
      "THRESH: 0.001\n",
      "Percent without any missed: 0.9981549815498155\n",
      "Average number of constants per problem: 12.17490774907749\n",
      "\n",
      "THRESH: 0\n",
      "Percent without any missed: 1.0\n",
      "Average number of constants per problem: 24.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.5, 0.05, 0.005, 0.001, 0]\n",
    "for thresh in thresholds:\n",
    "    print(f'THRESH: {thresh}')\n",
    "    avg_per_prob, percent_no_missed = percent_without_missed_const(predictions['validation'].predictions, predictions['validation'].label_ids, thresh)\n",
    "    print(f\"Percent without any missed: {percent_no_missed}\")\n",
    "    print(f\"Average number of constants per problem: {avg_per_prob}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4848c27",
   "metadata": {},
   "source": [
    "To show that this generalizes, showing the test set using a .005 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0debf886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THRESH: 0.005\n",
      "Percent without any missed: 0.9805339265850945\n",
      "Average number of constants per problem: 5.800889877641824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds = [.005]\n",
    "for thresh in thresholds:\n",
    "    print(f'THRESH: {thresh}')\n",
    "    avg_per_prob, percent_no_missed = percent_without_missed_const(predictions['test'].predictions, predictions['test'].label_ids, thresh)\n",
    "    print(f\"Percent without any missed: {percent_no_missed}\")\n",
    "    print(f\"Average number of constants per problem: {avg_per_prob}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54606404",
   "metadata": {},
   "source": [
    "## Storing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1b008",
   "metadata": {},
   "source": [
    "Lastly, the the predicted values along with a mapping are saved to disk for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c7fad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(p, thresh=0.005):\n",
    "    # Converting all values in the vector to be between 0 and 1\n",
    "    # The reason why softmax is not used is because we are doing multi label classification, meaning the total sum may be above 1\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    prob = sigmoid(torch.Tensor(p))\n",
    "    \n",
    "    # Converting items above the threshold to integers\n",
    "    y_pred = np.zeros(prob.shape)\n",
    "    y_pred[np.where(prob >= thresh)] = 1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "y_pred = {name:get_pred(predictions[name].predictions) for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd7d6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'pred': y_pred, 'map':const2id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "088af01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(FINAL_DIR):\n",
    "    os.makedirs(FINAL_DIR)\n",
    "    \n",
    "with open(f'{FINAL_DIR}constants.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a060f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
