{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a5bb58",
   "metadata": {},
   "source": [
    "# MathQA Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a6a20",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "- As of now, this model does not account for a problem having more than one of the same number (these numbers would have different embeddings as they are from different locations in the problem text). Consider implementing this in the future.\n",
    "- This also does not account for problems that have operations not in \\{+.-,*,/,^\\} so also consider adding more operations if performance is acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806a42d",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Figure out how to get true labels\n",
    "- Write custom loss function\n",
    "- test 1 full pass of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10e4a1",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fffb4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os\n",
    "import anytree\n",
    "from anytree import RenderTree\n",
    "from anytree.importer import DictImporter\n",
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9991e",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d428a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "MAX_LAYERS = 8\n",
    "MAX_TOKENS = 392\n",
    "EMBEDDING_SIZE = 768\n",
    "\n",
    "DATA_PATH = './dataset/'\n",
    "SET_NAMES = ['train', 'validation', 'test']\n",
    "ENCODER_MODEL = 'distilroberta-base' # A more optimized version of roberta obtaining 95% of its performance\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_MASK = '<num>'\n",
    "WORKING_DIR = 'TEMP/'\n",
    "\n",
    "OBJ_DIR = 'pickle/'\n",
    "\n",
    "\n",
    "class Op(Enum):\n",
    "    ADD = '+'\n",
    "    SUB = '-'\n",
    "    MULT = '*'\n",
    "    DIV = '/'\n",
    "    POW = '^'\n",
    "    \n",
    "class Const(Enum):\n",
    "    CONST_NEG_1 = 'const_neg_1' # I added this\n",
    "    CONST_0_25 = 'const_0_25'\n",
    "    CONST_0_2778 = 'const_0_2778'\n",
    "    CONST_0_33 = 'const_0_33'\n",
    "    CONST_0_3937 = 'const_0_3937'\n",
    "    CONST_1 = 'const_1'\n",
    "    CONST_1_6 = 'const_1_6'\n",
    "    CONST_2 = 'const_2'\n",
    "    CONST_3 = 'const_3'\n",
    "    CONST_PI = 'const_pi'\n",
    "    CONST_3_6 = 'const_3_6'\n",
    "    CONST_4 = 'const_4'\n",
    "    CONST_5 = 'const_5'\n",
    "    CONST_6 = 'const_6'\n",
    "    CONST_10 = 'const_10'\n",
    "    CONST_12 = 'const_12'\n",
    "    CONST_26 = 'const_26'\n",
    "    CONST_52 = 'const_52'\n",
    "    CONST_60 = 'const_60'\n",
    "    CONST_100 = 'const_100'\n",
    "    CONST_180 = 'const_180'\n",
    "    CONST_360 = 'const_360'\n",
    "    CONST_1000 = 'const_1000'\n",
    "    CONST_3600 = 'const_3600'\n",
    "\n",
    "values = [-1, 0.25, 0.2778, 0.33, 0.3937, 1, 1.6, 2, 3, math.pi, 3.6, 4, 5, 6, 10, 12, 26, 52, 60, 100, 180, 360, 1000, 3600]\n",
    "const2val = {k:v for k,v in zip(Const._value2member_map_.keys(), values)}    \n",
    "\n",
    "op2id = {k:v for k,v in zip(Op._value2member_map_.keys(), range(len(Op._value2member_map_)))}\n",
    "op2id['None'] = 5\n",
    "const2id = {k:v for k,v in zip(Const._value2member_map_.keys(), range(len(Const._value2member_map_)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda851b7",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5bd5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {name:pd.read_csv(f'{DATA_PATH}{name}.csv') for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad07be",
   "metadata": {},
   "source": [
    "## Embedding Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a415f6",
   "metadata": {},
   "source": [
    "Converting number embeddings to masked tensors (to ensure they are all homogeneous) and adding in constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7736e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_masked_embeddings(name):\n",
    "#     num_embed = embeddings[name]['num']\n",
    "#     const_embed = embeddings[name]['num']\n",
    "#     mapping, nums = embeddings[name]['num_mapping']\n",
    "#     const = [[num for num in eval(x) if num in const2val] for x in data[name]['nums']]\n",
    "#     max_nums = (np.bincount(mapping)+np.array(list(map(len, const)))).max()\n",
    "\n",
    "#     result = () \n",
    "#     for idx in range(len(const)):\n",
    "#         # Getting number embeddings\n",
    "#         nums = num_embed[mapping==idx]\n",
    "\n",
    "#         # Adding constant embeddings\n",
    "#         c = tuple([const_embed[const2id[x]] for x in const[idx]])\n",
    "#         if len(c) > 0:\n",
    "#             c = torch.stack(c)\n",
    "#             nums = torch.cat((nums, c), dim=0)\n",
    "        \n",
    "#           # non masked code\n",
    "# #         if result is None:\n",
    "# #             result = nums\n",
    "# #         else:\n",
    "# #             result = torch.cat((result, nums), dim=0)\n",
    "        \n",
    "#         # Padding and creating mask\n",
    "#         dim1, dim2 = nums.shape\n",
    "#         mask = torch.full((dim1,dim2), True)\n",
    "#         nums = F.pad(nums, (0,0,0,max_nums-dim1), 'constant', 0)\n",
    "#         mask = F.pad(mask, (0,0,0,max_nums-dim1), 'constant', False)\n",
    "        \n",
    "#         # Creating masked tensor object\n",
    "#         mt = torch.masked.masked_tensor(nums, mask)[None,:,:]\n",
    "#         result = result + (mt,)\n",
    "        \n",
    "#     return torch.cat(result, dim=0)\n",
    "# masked_num_embed = {name:create_masked_embeddings(name) for name in SET_NAMES}\n",
    "# masked_num_embed['train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081437d6",
   "metadata": {},
   "source": [
    "## Constructing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4c533b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerDecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, num_tokens, max_layers, K, device):\n",
    "        super(MultiLayerDecoder, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.max_layers = max_layers\n",
    "        self.K = K\n",
    "        self.decoder_layer = torch.nn.ModuleList([MathQADecoder(embedding_size, num_tokens, self.K, device) for x in range(max_layers-1)])\n",
    "        self.decoder_layer.insert(0, MathQADecoder(embedding_size, num_tokens, self.K+1, device, True)) # the first layer has K+1 tokens for the sos token\n",
    "        \n",
    "    def loss():\n",
    "        pass\n",
    "    \n",
    "    # Repeatedly get new sets of equations until a maximum depth is reached or there are no more valid equations (all ops are None)\n",
    "    def forward(self, x, embeddings):\n",
    "        \n",
    "        x = x.to(self.device)\n",
    "        nums = embeddings['nums'].to(self.device)\n",
    "        num_idx = embeddings['num_idx'].to(self.device)\n",
    "        ops = embeddings['ops'].to(self.device)\n",
    "        problems = embeddings['problem'].to(self.device)\n",
    "        \n",
    "        # variables to keep track of what equations have been encountered so far\n",
    "        init_dict = lambda x: {i:i for i in range(x)}\n",
    "        eq2id = np.array(list(map(init_dict, num_idx.bincount())))\n",
    "        id2eq = np.array(list(map(init_dict, num_idx.bincount())))\n",
    "\n",
    "        # while less than max layers and the input has more elements\n",
    "        idx = 0\n",
    "        temp = [] # REMOVE THIS\n",
    "        while idx < self.max_layers and x.numel():\n",
    "            batch_size = x.shape[0]\n",
    "            prev_idx = num_idx\n",
    "            prev_nums = nums\n",
    "            \n",
    "            # --------------------------------------------------\n",
    "            # Step 1 - Encoding to embedding size if not already\n",
    "            # --------------------------------------------------\n",
    "            if x.shape[-1] == self.embedding_size*4:\n",
    "                x = self.decoder_layer.exp_encoder(x) #3 [batch_size, K, 3072] -> [batch_size, K, 768]\n",
    "\n",
    "            # -----------------------\n",
    "            # Step 2 - The main model\n",
    "            # -----------------------\n",
    "            prev_sizes = num_idx.bincount()\n",
    "            x, op, num1, num2, nums, num_idx, problems, eq2id, id2eq = self.decoder_layer[idx](x, nums, num_idx, ops, \n",
    "                                                                                          problems, eq2id, id2eq)\n",
    "            new_sizes = num_idx.bincount()\n",
    "            \n",
    "            # ---------------------------------------------------------------------------------------------------------------------------------\n",
    "            # Step 3 - Removing examples with no valid expressions (If the num nums for a problem is unchanged, it had no valid expressions)\n",
    "            # ---------------------------------------------------------------------------------------------------------------------------------\n",
    "            changed = prev_sizes!=new_sizes # [batch_size] (not_finished True, finished False)\n",
    "            assert len(changed) == batch_size\n",
    "            not_finished = changed.sum()\n",
    "            assert not_finished <= batch_size\n",
    "            nums = nums[changed[num_idx]]  # [num_nums_not_finished, 768]\n",
    "            assert nums.shape[0] <= len(num_idx) and nums.shape[1] == self.embedding_size\n",
    "            prev = num_idx\n",
    "            num_idx = num_idx[changed[num_idx]] # [num_nums_not_finished]\n",
    "            num_idx = torch.unique(num_idx,return_inverse=True)[1]\n",
    "            assert len(num_idx) <= len(prev)\n",
    "            problems = problems[changed] # [not_finished, num_tokens, 768]\n",
    "            assert problems.shape == torch.Size([not_finished, self.num_tokens, self.embedding_size])\n",
    "            eq2id = eq2id[changed] # [not_finished,]\n",
    "            id2eq = id2eq[changed] # [not_finished,]\n",
    "            assert len(eq2id) == not_finished and len(id2eq) == not_finished\n",
    "            x = x[changed] # [not_finished, K, 768]\n",
    "            assert x.shape == torch.Size([not_finished, self.K, self.embedding_size])\n",
    "\n",
    "            idx += 1\n",
    "            temp.append((op,num1,num2,prev_idx,prev_nums,eq2id,id2eq))\n",
    "\n",
    "        return temp\n",
    "\n",
    "        #return op, num1, num2, prev_idx\n",
    "        \n",
    "\n",
    "class MathQADecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, num_tokens, K, device, first=False): \n",
    "        super(MathQADecoder, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.first = first\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.K = K\n",
    "        \n",
    "        # decreasing dimensionality to match embedding size: <op, num, num, num*num> = 3072 -> 768\n",
    "        self.exp_encoder = torch.nn.Linear(embedding_size*4, embedding_size)\n",
    "        \n",
    "        # converting back into new <op, num, num> for loss calculation\n",
    "        self.exp_decoder = torch.nn.Linear(embedding_size, embedding_size*3)\n",
    "        \n",
    "        # Mixing in expression information to the problem encoding\n",
    "        if first:\n",
    "            self.prob_encoder = torch.nn.Linear(num_tokens+self.K-1, num_tokens)\n",
    "        else:\n",
    "            self.prob_encoder = torch.nn.Linear(num_tokens+self.K, num_tokens)\n",
    "        \n",
    "        # standard transformer decoder\n",
    "        # we choose K heads, K layers for K generated expressions \n",
    "        nhead = self.K\n",
    "        num_layers = self.K\n",
    "        if self.first: # The first layer has K+1 tokens (one for the sos token)\n",
    "            nhead-=1\n",
    "            num_layers-=1\n",
    "            self.remove_excess_token = torch.nn.Linear(self.K, self.K-1)\n",
    "        # (with the hope that each head/layer will get different information for each K expression)\n",
    "        transformer_decoder_layer = torch.nn.TransformerDecoderLayer(embedding_size, nhead=nhead, batch_first=True)\n",
    "        self.transformer_decoder = torch.nn.TransformerDecoder(transformer_decoder_layer, num_layers=num_layers)\n",
    "        self.tgt_mask = torch.nn.Transformer().generate_square_subsequent_mask(sz=self.K) # prevents leftward attention\n",
    "        \n",
    "        # softmax for the operations\n",
    "        self.op_softmax = torch.nn.Softmax(dim=2)\n",
    "        \n",
    "        # softmax for the numbers\n",
    "        self.num_softmax = torch.nn.Softmax(dim=0)\n",
    "        \n",
    "    def __apply_to_nums(self, f, nums, num_idx, batch_size):\n",
    "        new_nums = torch.empty(nums.shape)\n",
    "        for x in range(batch_size):\n",
    "            new_nums[num_idx==x] = f(nums[num_idx==x])\n",
    "        return new_nums\n",
    "    \n",
    "    def __update_labels(self, eq2id, id2eq, mask, problem_idx, num1_ids, num2_ids, batch_size, num_idx):\n",
    "        num_valid = len(problem_idx)\n",
    "        valid1 = num1_ids[mask] # [num_valid,]\n",
    "        valid2 = num2_ids[mask] # [num_valid,]\n",
    "        assert len(valid1) == num_valid and len(valid2) == num_valid\n",
    "        for p in range(batch_size):\n",
    "            assert max(id2eq[p].keys()) == (num_idx==p).sum().item()-1\n",
    "            for idx, e in zip(np.where(mask[p])[0], zip(valid1[problem_idx==p], valid2[problem_idx==p])):\n",
    "                assert e[0].item() in id2eq[p] and e[1].item() in id2eq[p]\n",
    "                e = (id2eq[p][e[0].item()],id2eq[p][e[1].item()])\n",
    "                    \n",
    "                if e not in eq2id[p]:\n",
    "                    next_idx = len(eq2id[p])\n",
    "                    eq2id[p][e] = next_idx\n",
    "                    id2eq[p][next_idx] = e\n",
    "                else: # duplicate eq\n",
    "                    mask[p,idx] = False\n",
    "        return eq2id, id2eq, mask\n",
    "      \n",
    "    # x: [batch_size, K, 768]\n",
    "    # nums: [num_nums, 768]\n",
    "    # num_idx: [num_nums,]\n",
    "    # ops: [num_ops, 768]\n",
    "    # problems: [batch_size, num_tokens, 768]\n",
    "    def forward(self, x, nums, num_idx, ops, problems, eq2id, id2eq):    \n",
    "        batch_size = x.shape[0]\n",
    "        num_ops = ops.shape[0]\n",
    "        num_nums = nums.shape[0]\n",
    "\n",
    "        # ----------------------------\n",
    "        # Step 1 - transformer decoder\n",
    "        # ----------------------------\n",
    "        assert problems.shape == torch.Size([batch_size, self.num_tokens, self.embedding_size])\n",
    "        assert x.shape == torch.Size([batch_size, self.K, self.embedding_size])\n",
    "        x = self.transformer_decoder(x, problems, tgt_mask=self.tgt_mask) # [batch_size, K, 768] -> [batch_size, K, 768] (problems is [batch_size, num_tokens, 768])\n",
    "        if self.first:\n",
    "            x = self.remove_excess_token(x.permute(0,2,1))\n",
    "            x = x.permute(0,2,1)\n",
    "        assert x.shape == torch.Size([batch_size, K, self.embedding_size])\n",
    "        \n",
    "        # -------------------------------------------------------------------------------\n",
    "        # Step 2 - decoding the output into three embeddings of size 768 (op, num1, num2)\n",
    "        # -------------------------------------------------------------------------------\n",
    "        x = self.exp_decoder(x) # [batch_size, K, 768] -> [batch_size, K, 2304]\n",
    "        assert x.shape == torch.Size([batch_size, K, self.embedding_size*3])\n",
    "        operation, x1, x2 = torch.split(x, self.embedding_size, dim=2) # [batch_size, K, 2304] -> [batch_size, K, 768] for each\n",
    "        assert operation.shape == torch.Size([batch_size, K, self.embedding_size])\n",
    "        assert x1.shape == torch.Size([batch_size, K, self.embedding_size])\n",
    "        assert x2.shape == torch.Size([batch_size, K, self.embedding_size])\n",
    "\n",
    "        # -----------------------------------------------------------------------------------------------\n",
    "        # Step 3 and 4 - Finding the softmax for the similarity between the predicted and true embeddings\n",
    "        # -----------------------------------------------------------------------------------------------\n",
    "        # making sure params have correct dimension\n",
    "        nums_expanded = nums[:,None,:].expand(-1,K,-1) # [number_of_nums, 768] -> [number_of_nums, K, 768]\n",
    "        assert nums_expanded.shape == torch.Size([num_nums, K, self.embedding_size])\n",
    "        ops = ops[None,None,:,:].repeat(batch_size,K,1,1) # [num_ops, 768] -> [batch_size, K, num_ops, 768]\n",
    "        assert ops.shape == torch.Size([batch_size, K, num_ops, self.embedding_size])\n",
    "\n",
    "        # dot product - calculating the similarity between each op/num prediction and stored embeddings\n",
    "        num1 = (x1[num_idx]*nums_expanded).sum(dim=2) # [number_of_nums, K]\n",
    "        assert num1.shape == torch.Size([num_nums, K])\n",
    "        num2 = (x2[num_idx]*nums_expanded).sum(dim=2) # [number_of_nums, K]        \n",
    "        assert num2.shape == torch.Size([num_nums, K])\n",
    "        op = operation[:,:,None,:].expand(-1,-1,num_ops,-1) # [batch_size, K, 768] -> # [batch_size, K, num_ops, 768]\n",
    "        assert op.shape == torch.Size([batch_size, K, num_ops, self.embedding_size])\n",
    "        op = (op*ops).sum(dim=3) # [batch_size, K, num_ops, 768] -> [batch_size, K, num_ops]\n",
    "        assert op.shape == torch.Size([batch_size, K, num_ops])\n",
    "\n",
    "        # softmax\n",
    "        op = self.op_softmax(op) # [batch_size, K, num_ops] (ie op[1,2] would be the operator prediction probabilities for problem2, query3)\n",
    "        assert op.shape == torch.Size([batch_size, K, num_ops])\n",
    "        assert op.sum(dim=2).sum()==batch_size*K\n",
    "        \n",
    "        num1 = self.__apply_to_nums(self.num_softmax, num1, num_idx, batch_size) # [num_nums, K]\n",
    "        assert num1.shape == torch.Size([num_nums, K])\n",
    "        assert np.isclose(num1[num_idx==0][:,0].sum().item(),1)\n",
    "        \n",
    "        num2 = self.__apply_to_nums(self.num_softmax, num2, num_idx, batch_size) # [num_nums, K]\n",
    "        assert num2.shape == torch.Size([num_nums, K])\n",
    "        assert np.isclose(num2[num_idx==0][:,0].sum().item(),1)\n",
    "        \n",
    "        # ----------------------------------------------------\n",
    "        # Step 5 - creating embedding for the found expression  \n",
    "        # ----------------------------------------------------\n",
    "        x = self.exp_encoder(torch.cat((operation,x1,x2,x1*x2), dim=2)) # [batch_size, K, 3072] -> [batch_size, K, 768]  \n",
    "        assert x.shape == torch.Size([batch_size, K, self.embedding_size])\n",
    "\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "        # Step 6 - finding valid, adding found expression to problem embeddings and number embeddings\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "        # Getting predicted ops/nums and creating a problem index for what problem they refer to\n",
    "        op_ids = torch.argmax(op, dim=2) # [batch_size, K]\n",
    "        assert op_ids.shape == torch.Size([batch_size, K])\n",
    "        assert op_ids.max() < num_ops and op_ids.min() >= 0\n",
    "        mask = op_ids!=op2id['None'] # [batch_size, K]\n",
    "        assert mask.shape == torch.Size([batch_size, K])\n",
    "        num1_ids = torch.stack(tuple([num1[num_idx==x].argmax(0) for x in range(batch_size)])) # [batch_size, K]\n",
    "        assert num1_ids.shape == torch.Size([batch_size, K])\n",
    "        assert num1_ids.max() < num_idx[num_idx==num1_ids.argmax()//K].shape[0] and num1_ids.min() >= 0\n",
    "        num2_ids = torch.stack(tuple([num2[num_idx==x].argmax(0) for x in range(batch_size)])) # [batch_size, K]\n",
    "        assert num2_ids.shape == torch.Size([batch_size, K])\n",
    "        assert num2_ids.max() < num_idx[num_idx==num2_ids.argmax()//K].shape[0] and num2_ids.min() >= 0\n",
    "        problem_idx = torch.arange(mask.shape[0]).to(self.device).repeat_interleave(K)[mask.flatten()] # [num_valid,]\n",
    "        num_valid = mask.sum()\n",
    "        assert len(problem_idx) == num_valid\n",
    "        \n",
    "        # Figuring out and keeping track of what equations each label corresponds to (to avoid duplicates/for loss calculation)\n",
    "        eq2id, id2eq, mask = self.__update_labels(eq2id, id2eq, mask, problem_idx, num1_ids, num2_ids, batch_size, num_idx)\n",
    "        assert len(eq2id) == batch_size and len(id2eq) == batch_size\n",
    "\n",
    "        # Getting valid embeddings along with an updated problem index\n",
    "        num_valid = mask.sum()\n",
    "        valid = x[mask] # [num_valid, 768]\n",
    "        assert valid.shape == torch.Size([num_valid, 768])\n",
    "        problem_idx = torch.arange(mask.shape[0]).to(self.device).repeat_interleave(K)[mask.flatten()] # [num_valid,]\n",
    "        assert len(problem_idx) == num_valid\n",
    "\n",
    "        # Appending to num_embeddings\n",
    "        nums = torch.cat((nums, valid), dim=0) # [num_nums+num_valid, 768]\n",
    "        assert nums.shape == torch.Size([num_valid+num_nums, self.embedding_size])\n",
    "        num_idx = torch.cat((num_idx, problem_idx), dim=0) # [num_nums+num_valid,]\n",
    "        assert len(num_idx) == num_valid+num_nums\n",
    "\n",
    "        # Updating problem embeddings\n",
    "        temp = torch.clone(x)\n",
    "        temp[~mask] = 0 # [batch_size, K, 768] # CONSIDER CHANGING THIS (rn just setting invalid to zero)\n",
    "        assert temp.shape == torch.Size([batch_size, K, 768])\n",
    "        problems = torch.cat((problems,temp),dim=1) # [batch_size, num_tokens+K, 768]\n",
    "        assert problems.shape == torch.Size([batch_size, self.num_tokens+K, self.embedding_size])\n",
    "        problems = self.prob_encoder(problems.permute(0,2,1)) # [batch_size, 768, num_tokens]\n",
    "        assert problems.shape == torch.Size([batch_size, self.embedding_size, self.num_tokens])\n",
    "        problems = problems.permute(0,2,1) # [batch_size, num_tokens, 768]\n",
    "        assert problems.shape == torch.Size([batch_size, self.num_tokens, self.embedding_size])\n",
    "        \n",
    "        # -----------------------\n",
    "        # Returning final results\n",
    "        # -----------------------\n",
    "        return x, op, num1, num2, nums, num_idx, problems, eq2id, id2eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ea8221de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: [batch_size, K, 768]\n",
    "# nums: [num_nums, 768]\n",
    "# num_idx: [num_nums,]\n",
    "# ops: [num_ops, 768]\n",
    "# problems: [batch_size, num_tokens, 768]\n",
    "try:\n",
    "    seed = 3\n",
    "    with open(f'{OBJ_DIR}embeddings/train/batch0.pickle', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    x = torch.rand(8,K+1,768)\n",
    "    model = MultiLayerDecoder(768, 392, 8, 6, 'cpu')\n",
    "    #model.to('cuda:0')\n",
    "    result = model(x, embeddings)\n",
    "finally:\n",
    "    #del embeddings\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "11664bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 6])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Linear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
