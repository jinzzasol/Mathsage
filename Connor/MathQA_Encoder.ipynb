{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b949f5ef",
   "metadata": {},
   "source": [
    "# MathQA Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8417205d",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e92b04d3-cd66-4237-b824-ac18a7debedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os\n",
    "import anytree\n",
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d48fc",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f00c0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './dataset/'\n",
    "SET_NAMES = ['train', 'validation', 'test']\n",
    "ENCODER_MODEL = 'distilroberta-base' # A more optimized version of roberta obtaining 95% of its performance\n",
    "MAX_TOKENS = 392\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_MASK = '<num>'\n",
    "WORKING_DIR = 'TEMP/'\n",
    "FINAL_DIR = 'embeddings/'\n",
    "\n",
    "class Op(Enum):\n",
    "    ADD = '+'\n",
    "    SUB = '-'\n",
    "    MULT = '*'\n",
    "    DIV = '/'\n",
    "    POW = '^'\n",
    "    \n",
    "class Const(Enum):\n",
    "    CONST_NEG_1 = 'const_neg_1' # I added this\n",
    "    CONST_0_25 = 'const_0_25'\n",
    "    CONST_0_2778 = 'const_0_2778'\n",
    "    CONST_0_33 = 'const_0_33'\n",
    "    CONST_0_3937 = 'const_0_3937'\n",
    "    CONST_1 = 'const_1'\n",
    "    CONST_1_6 = 'const_1_6'\n",
    "    CONST_2 = 'const_2'\n",
    "    CONST_3 = 'const_3'\n",
    "    CONST_PI = 'const_pi'\n",
    "    CONST_3_6 = 'const_3_6'\n",
    "    CONST_4 = 'const_4'\n",
    "    CONST_5 = 'const_5'\n",
    "    CONST_6 = 'const_6'\n",
    "    CONST_10 = 'const_10'\n",
    "    CONST_12 = 'const_12'\n",
    "    CONST_26 = 'const_26'\n",
    "    CONST_52 = 'const_52'\n",
    "    CONST_60 = 'const_60'\n",
    "    CONST_100 = 'const_100'\n",
    "    CONST_180 = 'const_180'\n",
    "    CONST_360 = 'const_360'\n",
    "    CONST_1000 = 'const_1000'\n",
    "    CONST_3600 = 'const_3600'\n",
    "\n",
    "values = [-1, 0.25, 0.2778, 0.33, 0.3937, 1, 1.6, 2, 3, math.pi, 3.6, 4, 5, 6, 10, 12, 26, 52, 60, 100, 180, 360, 1000, 3600]\n",
    "const2val = {k:v for k,v in zip(Const._value2member_map_.keys(), values)}    \n",
    "\n",
    "op2id = {k:v for k,v in zip(Op._value2member_map_.keys(), range(len(Op._value2member_map_)))}\n",
    "const2id = {k:v for k,v in zip(Const._value2member_map_.keys(), range(len(Const._value2member_map_)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5120984",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528e98f",
   "metadata": {},
   "source": [
    "Reading csv into a dictionary of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1239b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {name:pd.read_csv(f'{DATA_PATH}{name}.csv') for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cfd60",
   "metadata": {},
   "source": [
    "Converts operations for each problem into a multi label onehot encoded setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe3a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_ops(data):\n",
    "    labels = []\n",
    "    for op_set in data.ops:\n",
    "        op_set = eval(op_set)\n",
    "        idx = [op2id[op] for op in op_set]\n",
    "        onehot = np.zeros(len(op2id))\n",
    "        onehot[idx] = 1\n",
    "        labels.append(onehot)\n",
    "    return np.array(labels)\n",
    "        \n",
    "#onehot_ops(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93df24d",
   "metadata": {},
   "source": [
    "Sort nums for each each problem in increasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e7d18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_num(nums):\n",
    "    get_float = lambda x: float(const2val[x]) if x in const2val else float(x)\n",
    "    return max(map(get_float, nums))\n",
    "\n",
    "def remove_const(data):\n",
    "    nums = []\n",
    "    for num_list in data.nums:\n",
    "        nums.append(set([float(x) for x in eval(num_list) if x not in const2val]))\n",
    "    return nums\n",
    "\n",
    "# Gets the numbers listed in a problem\n",
    "# Once found, numbers are masked using a number mask\n",
    "def get_nums_from_problem(data, convert_to_float=False):\n",
    "    nums = []\n",
    "    problems = []\n",
    "    for problem in data.problem:\n",
    "        num = re.compile('([+-]?((\\d+(\\.\\d*)?)|(\\.\\d+)))')\n",
    "        big = re.compile(r'(-?\\d{1,3}(,\\d{3})+(\\.\\d*)?)')\n",
    "        \n",
    "        big_results = re.finditer(big, problem)\n",
    "        problem = re.sub(big, NUM_MASK, problem)        \n",
    "        num_results = re.finditer(num, problem)\n",
    "        problem = re.sub(num, NUM_MASK, problem)\n",
    "        \n",
    "        # Getting the combined numbers in order of occurence\n",
    "        combined = [x for x in num_results]\n",
    "        combined.extend([x for x in big_results])\n",
    "        combined = sorted(combined, key=lambda x: x.start(0))\n",
    "        \n",
    "        if convert_to_float:\n",
    "            combined = [float(x.group(0).replace(',','')) for x in combined]\n",
    "        else:\n",
    "            combined = [x.group(0) for x in combined]\n",
    "        \n",
    "        nums.append(combined)\n",
    "        problems.append(problem)\n",
    "    return nums, problems\n",
    "\n",
    "def sort_nums(data):\n",
    "    nums_sorted = []\n",
    "    nums_no_const_sorted = []\n",
    "    for nums in data.nums_no_const:\n",
    "        nums_no_const_sorted.append(sorted(list(eval(nums)), key=lambda x: float(x)))\n",
    "    for nums in data.nums:\n",
    "        num_list = list(eval(nums))\n",
    "        maximum = max_num(num_list)\n",
    "        get_float = lambda x: float(const2val[x])+maximum if x in const2val else float(x)\n",
    "        nums_sorted.append(sorted(num_list, key=get_float))\n",
    "    return nums_sorted, nums_no_const_sorted\n",
    "\n",
    "#sort_nums(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7260430",
   "metadata": {},
   "source": [
    "Here I do some testing to see if the numbers from the equation can be found in the problem description using simple regexes. This actually works extremely well, having no examples where the expected numbers is not a subset of the obtained numbers. This does not include constants. Constants are values which should not occur in the problem description (like pi or the 2 in r^2 for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43603d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = remove_const(data['train'])\n",
    "obtained,_ = get_nums_from_problem(data['train'], convert_to_float=True)\n",
    "obtained = [set(x) for x in obtained]\n",
    "\n",
    "idx = 0\n",
    "for x, y in zip(expected, obtained):\n",
    "    if not (x <= y):\n",
    "        print('------------------')\n",
    "        print(data['train']['problem'][idx])\n",
    "        print(f'Expected: {x}')\n",
    "        print(f'Obtained: {y}')\n",
    "        print('------------------')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe01d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "general        7231\n",
       "physics        4908\n",
       "gain           3544\n",
       "geometry       1422\n",
       "other          1071\n",
       "probability     145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bc1ac-89a5-4e66-a781-4595950ab9f4",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfc7b7-7194-4dcd-8423-99080999cd25",
   "metadata": {},
   "source": [
    "In this step, we use Roberta to get contextualized embeddings for each math problem\n",
    "\n",
    "First, the problem texts must be tokenized into input ids. A number mask token is used for each number in the problem, as they should not affect the problem itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f7585ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at distilroberta-base-encoder-mathqa and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = f'{ENCODER_MODEL}-encoder-mathqa'\n",
    "#model = AutoModelForMaskedLM.from_pretrained(ENCODER_MODEL) # Used for fine tuning only\n",
    "model = AutoModel.from_pretrained(model_path, output_hidden_states=True) # Fine tuned model used for getting the contextualized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd053ea-161e-44a7-bb13-74e4ca6a0c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50266. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ENCODER_MODEL)\n",
    "\n",
    "# Adding a new token to the model, for masking out numbers.\n",
    "tokenizer.add_special_tokens({'additional_special_tokens':[NUM_MASK]})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def tokenize_data(data):\n",
    "    tokenization = lambda x: tokenizer(x, padding='max_length', max_length=MAX_TOKENS, truncation=True)\n",
    "    _,problem = get_nums_from_problem(data)\n",
    "    \n",
    "    tokenized = list(map(tokenization, problem))\n",
    "    input_ids = torch.stack([torch.tensor(x['input_ids']) for x in tokenized])\n",
    "    attention_mask = torch.stack([torch.tensor(x['attention_mask']) for x in tokenized])\n",
    "    \n",
    "    return {'input_ids':input_ids.long(), 'attention_mask':attention_mask.int()}\n",
    "\n",
    "tokenized = {name:tokenize_data(data[name]) for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fabc7e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of problems that exceed 392 tokens: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of problems that exceed {MAX_TOKENS} tokens: {np.sum(np.array((tokenized['train']['input_ids'][:,-1]!=1)))}\") # 1 is the padding token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b6185",
   "metadata": {},
   "source": [
    "Next, the encoder model is finetuned on MathQA, using masked language modeling, similar to how bert does its trainined. This allows the model to create better contextualized representations for each math problem. Hyperparameters courtesy of https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb#scrollTo=QRTpmyCc3l_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d749960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = TrainingArguments(\n",
    "#     f'{WORKING_DIR}{model_path}',\n",
    "#     evaluation_strategy='epoch',\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     per_device_train_batch_size = 8,\n",
    "#     per_device_eval_batch_size = 8,\n",
    "# )\n",
    "\n",
    "# train = Dataset.from_dict(tokenized['train'])\n",
    "# val = Dataset.from_dict(tokenized['validation'])\n",
    "# train.set_format('torch')\n",
    "# val.set_format('torch')\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=train,\n",
    "#     eval_dataset=val,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15) # using the masked probability from BERT\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796172ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada0709",
   "metadata": {},
   "source": [
    "This function gets the number index for each masked number token in the tokenized problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf5610d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_idx(tokenized):\n",
    "    mask_id = tokenizer.encode(NUM_MASK, add_special_tokens=False)[0]\n",
    "    ids = tokenized['input_ids']\n",
    "    \n",
    "    return [np.where(id==mask_id)[0] for id in ids]\n",
    "\n",
    "masked_idx = {name:get_masked_idx(tokenized[name]) for name in data.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a560e57",
   "metadata": {},
   "source": [
    "This function gets the problem indices that each constant is used in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f77f30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_const_problems(data):\n",
    "    const2idx = {const:[] for const in Const._value2member_map_.keys()}\n",
    "    for idx, num_list in enumerate(data.nums):\n",
    "        for x in eval(num_list):\n",
    "            if x in const2val:\n",
    "                const2idx[x].append(idx)\n",
    "    return {k:np.array(v) for k,v in const2idx.items()}\n",
    "\n",
    "const2idx = {name:get_const_problems(data[name]) for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f6413",
   "metadata": {},
   "source": [
    "Here the contextualized embeddings are obtained using the fientuned roberta model for the problem, problem numbers, and constants. The contextualized embeddings are just the sum of the last four hidden layers outputted from bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b9b126ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches a non homogeneous array given a number of splits\n",
    "def non_homogeneous_split(arr, num_per_batch):\n",
    "    return [arr[idx:idx+num_per_batch] for idx in range(0,len(arr),num_per_batch)]\n",
    "\n",
    "# Batches the const2idx dictionary\n",
    "def batch_const2idx(const2idx, name):\n",
    "    batched_const2idx = [{const:[] for const in Const._value2member_map_.keys()} for x in range(num_splits)]\n",
    "    split_size = math.ceil(len(data[name])/num_splits)\n",
    "    for k,v in const2idx[name].items():\n",
    "        for batch_num, batch_idx in zip(v//96, v%96):\n",
    "            batched_const2idx[batch_num][k].append(batch_idx)\n",
    "    return [{k:np.array(v) for k,v in x.items()} for x in batched_const2idx]\n",
    "\n",
    "# Putting model on gpu\n",
    "model.to(DEVICE)\n",
    "\n",
    "def get_embeddings(name):\n",
    "    # The final embeddings\n",
    "    embeddings = {'problem':None, 'num':[], 'const':None}\n",
    "\n",
    "    # batching ids and masks\n",
    "    num_per_batch = 100\n",
    "    num_splits = math.ceil(len(tokenized[name]['input_ids'])/num_per_batch)\n",
    "    batched_ids = torch.split(tokenized[name]['input_ids'], num_per_batch)\n",
    "    batched_masks = torch.split(tokenized[name]['attention_mask'], num_per_batch)\n",
    "    batched_idx = non_homogeneous_split(masked_idx[name], num_per_batch)\n",
    "\n",
    "    for batch_num in range(num_splits):\n",
    "        # Getting first batch and putting on gpu\n",
    "        ids = batched_ids[batch_num].to(DEVICE)\n",
    "        mask = batched_masks[batch_num].to(DEVICE)\n",
    "        idx = batched_idx[batch_num]\n",
    "\n",
    "        # Getting the raw hidden layer output\n",
    "        with torch.no_grad():\n",
    "            output = model(ids, mask)\n",
    "\n",
    "        # [batch_size * tokens * 13 * 768]\n",
    "        output = torch.stack(output[2], dim=0).permute(1,2,0,3)\n",
    "\n",
    "        # Summing the last 4 hidden layers from roberta to be used as the contextualized embeddings\n",
    "        output = torch.sum(output[:,:,-4:,:], dim=2)\n",
    "\n",
    "        # Getting the num embeddings at the index of each masked number\n",
    "        num_embeddings = [output[x,:,:][idx[x]].to('cpu') for x in range(len(idx))]\n",
    "        embeddings['num'].extend(num_embeddings)\n",
    "\n",
    "        # Getting the problem embeddings (Using the CLS (<s> for roberta) token to get a representation of the whole text)\n",
    "        problem_embeddings = output[:,0,:].to('cpu')\n",
    "        if embeddings['problem'] is None:\n",
    "            embeddings['problem'] = problem_embeddings\n",
    "        else:\n",
    "            embeddings['problem'] = torch.cat((embeddings['problem'], problem_embeddings), dim=0)\n",
    "            \n",
    "\n",
    "        # Cleaning up for the next batch\n",
    "        del ids\n",
    "        del mask\n",
    "        del idx\n",
    "        del output\n",
    "        torch.cuda.empty_cache()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a1b7c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {name:get_embeddings(name) for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fab2e6",
   "metadata": {},
   "source": [
    "To get the constant embeddings, we take the average of all of the problem embeddings that the constant was used in. This should hopefully give the constants some more context during downstream training. The training data is only used for the constant embeddings, as you would not know what constants belong to the problem in the test/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0149410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_const_embeddings():\n",
    "    name = 'train'\n",
    "    return torch.stack(tuple([torch.mean(embeddings[name]['problem'][const2idx[name][k]], dim=0) for k in const2idx[name].keys()]))\n",
    "\n",
    "embeddings['train']['const'] = get_const_embeddings()\n",
    "embeddings['validation']['const'] = get_const_embeddings()\n",
    "embeddings['test']['const'] = get_const_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf7dd3",
   "metadata": {},
   "source": [
    "This functions creates a problem index so that the numbers and embeddings for each number can be flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9db92752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nums_mapping(name):\n",
    "    nums,_ = get_nums_from_problem(data[name])\n",
    "    problem_idx = np.concatenate(tuple([np.full(len(num), idx) for idx, num in enumerate(nums)]))\n",
    "    problem_nums = np.concatenate(tuple(nums))\n",
    "    return problem_idx, problem_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216b501",
   "metadata": {},
   "source": [
    "Flattening the num embeddigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1f4f3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['train']['num'] = torch.cat(tuple(embeddings['train']['num']), dim=0)\n",
    "embeddings['validation']['num'] = torch.cat(tuple(embeddings['validation']['num']), dim=0)\n",
    "embeddings['test']['num'] = torch.cat(tuple(embeddings['test']['num']), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18057d05",
   "metadata": {},
   "source": [
    "Adding the num mapping to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b6c17334",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['train']['num_mapping'] = get_nums_mapping('train')\n",
    "embeddings['validation']['num_mapping'] = get_nums_mapping('validation')\n",
    "embeddings['test']['num_mapping'] = get_nums_mapping('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149152e9",
   "metadata": {},
   "source": [
    "Adding a mapping for the constants to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7d84c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['train']['const_mapping'] = const2id\n",
    "embeddings['validation']['const_mapping'] = const2id\n",
    "embeddings['test']['const_mapping'] = const2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11740cd",
   "metadata": {},
   "source": [
    "Lastly, the embeddings dictionary is stored to disk to be used in future ipynb files\n",
    "\n",
    "To load this object simply do:\n",
    "\n",
    "```\n",
    "with open(f'{FINAL_DIR}embeddings.pickle', 'rb') as f:\n",
    "    embeddings = pickle.load(handle)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "55584596",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(FINAL_DIR):\n",
    "    os.makedirs(FINAL_DIR)\n",
    "    \n",
    "with open(f'{FINAL_DIR}embeddings.pickle', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f81ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
