{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b949f5ef",
   "metadata": {},
   "source": [
    "# MathQA Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8417205d",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92b04d3-cd66-4237-b824-ac18a7debedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os\n",
    "import anytree\n",
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d48fc",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f00c0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './dataset/'\n",
    "SET_NAMES = ['train', 'validation', 'test']\n",
    "MODEL = 'roberta-base'\n",
    "MAX_TOKENS = 128\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "class Op(Enum):\n",
    "    ADD = '+'\n",
    "    SUB = '-'\n",
    "    MULT = '*'\n",
    "    DIV = '/'\n",
    "    POW = '^'\n",
    "    \n",
    "class Const(Enum):\n",
    "    CONST_PI = 'const_pi'\n",
    "    CONST_NEG_1 = 'const_neg_1' # I added this\n",
    "    CONST_DEG_TO_RAD = 'const_deg_to_rad' # pi / 180 (There is only one example of this and its actually used incorrectly)\n",
    "    CONST_1 = 'const_1'\n",
    "    CONST_2 = 'const_2'\n",
    "    CONST_3 = 'const_3'\n",
    "    CONST_4 = 'const_4'\n",
    "    CONST_5 = 'const_5'\n",
    "    CONST_6 = 'const_6'\n",
    "    CONST_10 = 'const_10'\n",
    "    CONST_12 = 'const_12'\n",
    "    CONST_26 = 'const_26'\n",
    "    CONST_52 = 'const_52'\n",
    "    CONST_60 = 'const_60'\n",
    "    CONST_100 = 'const_100'\n",
    "    CONST_180 = 'const_180'\n",
    "    CONST_360 = 'const_360'\n",
    "    CONST_1000 = 'const_1000'\n",
    "    CONST_3600 = 'const_3600' \n",
    "    CONST_0_25 = 'const_0_25'\n",
    "    CONST_0_2778 = 'const_0_2778'\n",
    "    CONST_0_33 = 'const_0_33'\n",
    "    CONST_0_3937 = 'const_0_3937'\n",
    "    CONST_0_4535 = 'const_0_4535'\n",
    "    CONST_0_6 = 'const_0_6'\n",
    "    CONST_1_6 = 'const_1_6'\n",
    "    CONST_2_2046 = 'const_2_2046'\n",
    "    CONST_2_54 = 'const_2_54'\n",
    "    CONST_3_6 = 'const_3_6' \n",
    "    CONST_0dot25 = 'const_0.25'\n",
    "    CONST_0dot5 = 'const_0.5' \n",
    "    CONST_2dot0 = 'const_2.0'\n",
    "    CONST_3dot0 = 'const_3.0'\n",
    "    CONST_4dot0 = 'const_4.0'\n",
    "    CONST_60dot0 = 'const_60.0'\n",
    "    CONST_100dot0 = 'const_100.0'\n",
    "\n",
    "values = [math.pi, -1, math.pi/180, 1, 2, 3, 4, 5, 6, 10, 12, 26, 52, 60, 100, 180, 360, 1000, 3600,\n",
    "          0.25, 0.2778, 1/3, 0.3937, 0.4535, 0.6, 1.6, 2.2046, 2.54, 3.6, 0.25, 0.5, 2.0, 3.0, 4.0, 60.0, 100.0]\n",
    "const2val = {k:v for k,v in zip(Const._value2member_map_.keys(), values)}    \n",
    "const2val['const_0_5'] = 0.5\n",
    "\n",
    "op2id = {k:v for k,v in zip(Op._value2member_map_.keys(), range(len(Op._value2member_map_)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5120984",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528e98f",
   "metadata": {},
   "source": [
    "Reading csv into a dictionary of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1239b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {name:pd.read_csv(f'{DATA_PATH}{name}.csv') for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cfd60",
   "metadata": {},
   "source": [
    "Converts operations for each problem into a multi label onehot encoded setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe3a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_ops(data):\n",
    "    labels = []\n",
    "    for op_set in data.ops:\n",
    "        op_set = eval(op_set)\n",
    "        idx = [op2id[op] for op in op_set]\n",
    "        onehot = np.zeros(len(op2id))\n",
    "        onehot[idx] = 1\n",
    "        labels.append(onehot)\n",
    "    return np.array(labels)\n",
    "        \n",
    "#onehot_ops(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93df24d",
   "metadata": {},
   "source": [
    "Sort nums for each each problem in increasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7d18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_num(nums):\n",
    "    get_float = lambda x: float(const2val[x]) if x in const2val else float(x)\n",
    "    return max(map(get_float, nums))\n",
    "\n",
    "def remove_const(data):\n",
    "    nums = []\n",
    "    for num_list in data.nums:\n",
    "        nums.append(set([float(x) for x in eval(num_list) if x not in const2val]))\n",
    "    return nums\n",
    "\n",
    "def get_nums_from_problem(data, convert_to_float=True):\n",
    "    nums = []\n",
    "    for problem in data.problem:\n",
    "        num = re.compile('([+-]?((\\d+(\\.\\d*)?)|(\\.\\d+)))')\n",
    "        big = re.compile(r'(-?\\d{1,3}(,\\d{3})+(\\.\\d*)?)')\n",
    "        \n",
    "        big_results = re.findall(big, problem)\n",
    "        problem = re.sub(big, '', problem)        \n",
    "        num_results = re.findall(num, problem)\n",
    "\n",
    "        if convert_to_float:\n",
    "            s1 = set([float(x[0].replace(',','')) for x in big_results])\n",
    "            s2 = set([float(x[0]) for x in num_results])\n",
    "        else:\n",
    "            s1 = set([x[0] for x in big_results])\n",
    "            s2 = set([x[0] for x in num_results])\n",
    "        \n",
    "        nums.append(s1.union(s2))\n",
    "    return nums\n",
    "\n",
    "def sort_nums(data):\n",
    "    nums_sorted = []\n",
    "    nums_no_const_sorted = []\n",
    "    for nums in data.nums_no_const:\n",
    "        nums_no_const_sorted.append(sorted(list(eval(nums)), key=lambda x: float(x)))\n",
    "    for nums in data.nums:\n",
    "        num_list = list(eval(nums))\n",
    "        maximum = max_num(num_list)\n",
    "        get_float = lambda x: float(const2val[x])+maximum if x in const2val else float(x)\n",
    "        nums_sorted.append(sorted(num_list, key=get_float))\n",
    "    return nums_sorted, nums_no_const_sorted\n",
    "\n",
    "#sort_nums(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7260430",
   "metadata": {},
   "source": [
    "Here I do some testing to see if the numbers from the equation can be found in the problem description using simple regexes. This actually works extremely well, having no examples where the expected numbers is not a subset of the obtained numbers. This does not include constants. Constants are values which should not occur in the problem description (like pi or the 2 in r^2 for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43603d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = remove_const(data['train'])\n",
    "obtained = get_nums_from_problem(data['train'])\n",
    "\n",
    "idx = 0\n",
    "for x, y in zip(expected, obtained):\n",
    "    if not (x <= y):\n",
    "        print('------------------')\n",
    "        print(data['train']['problem'][idx])\n",
    "        print(f'Expected: {x}')\n",
    "        print(f'Obtained: {y}')\n",
    "        print('------------------')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe01d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "general        7231\n",
       "physics        4908\n",
       "gain           3544\n",
       "geometry       1422\n",
       "other          1071\n",
       "probability     145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bc1ac-89a5-4e66-a781-4595950ab9f4",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfc7b7-7194-4dcd-8423-99080999cd25",
   "metadata": {},
   "source": [
    "In this step, we use Roberta to get contextualized embeddings for each math problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c23bfb9-590f-4a6b-83fa-853d3bbd7a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Greater than 128: 22\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for x in data['train']['problem']:\n",
    "    lengths.append(len(x.split()))\n",
    "lengths = np.array(lengths)\n",
    "print(f'Num Greater than 128: {np.sum(lengths>128)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4dd053ea-161e-44a7-bb13-74e4ca6a0c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "encoder = AutoModel.from_pretrained(MODEL, output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def tokenize_data(data):\n",
    "    tokenization = lambda x: tokenizer(x, padding='max_length', max_length=MAX_TOKENS, truncation=True)\n",
    "    \n",
    "    tokenized = data['problem'].map(tokenization).tolist()\n",
    "    input_ids = torch.stack([torch.tensor(x['input_ids']) for x in tokenized])\n",
    "    attention_mask = torch.stack([torch.tensor(x['attention_mask']) for x in tokenized])\n",
    "    \n",
    "    return {'input_ids':input_ids.long(), 'attention_mask':attention_mask.int()}\n",
    "\n",
    "tokenized = {name:tokenize_data(data[name]) for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42187364-f0da-4cb2-b35d-f5aeec14d53c",
   "metadata": {},
   "source": [
    "The following code gets the number of problems in the training set that exceed 128 tokens. Based on the low number of problems that exceed this, 128 is a good number of maximum tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f16bb006-fb6e-4d9e-aaf5-83fb41896c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of problems that exceed 128 tokens: 41\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of problems that exceed 128 tokens: {np.sum(np.array(tokenized['train']['input_ids'][:,-1]!=1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db275236-cd9e-4cd2-809c-1971467acd1b",
   "metadata": {},
   "source": [
    "Getting the contextualized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6be5c9be-8e83-4918-b78b-08c01cace121",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.32 GiB. GPU 0 has a total capacty of 15.61 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 14.33 GiB memory in use. Of the allocated memory 13.55 GiB is allocated by PyTorch, and 82.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      4\u001b[0m output\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:844\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    835\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    837\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    838\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    839\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    843\u001b[0m )\n\u001b[0;32m--> 844\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    857\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:529\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    520\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    521\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    522\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    527\u001b[0m     )\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 529\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/.venv/lib64/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:236\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    233\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    239\u001b[0m     query_length, key_length \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], key_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.32 GiB. GPU 0 has a total capacty of 15.61 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 14.33 GiB memory in use. Of the allocated memory 13.55 GiB is allocated by PyTorch, and 82.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "encoder.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    tokenized['test']['input_ids'].to(DEVICE)\n",
    "    tokenized['test']['attention_mask'].to(DEVICE)\n",
    "    output = encoder(**tokenized['test'])\n",
    "torch.cuda.empty_cache()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f6c04394-dcd4-4289-beba-83ed7d257e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0110,  0.0662, -0.0402,  ..., -0.0765, -0.0393,  0.0051],\n",
       "         [ 0.0246, -0.1859, -0.0735,  ..., -0.3455,  0.1115,  0.1199],\n",
       "         [ 0.0491,  0.0213,  0.0921,  ..., -0.0817,  0.0128, -0.0482],\n",
       "         ...,\n",
       "         [ 0.0790,  0.0204,  0.0805,  ...,  0.0735,  0.0150,  0.0238],\n",
       "         [ 0.0790,  0.0204,  0.0805,  ...,  0.0735,  0.0150,  0.0238],\n",
       "         [ 0.0790,  0.0204,  0.0805,  ...,  0.0735,  0.0150,  0.0238]],\n",
       "\n",
       "        [[-0.0115,  0.0960, -0.0436,  ..., -0.0642, -0.0510, -0.0248],\n",
       "         [ 0.1513, -0.4553, -0.0328,  ..., -0.1751,  0.3167, -0.2626],\n",
       "         [ 0.2684, -0.0636,  0.1094,  ..., -0.0759,  0.2083, -0.1840],\n",
       "         ...,\n",
       "         [ 0.1074,  0.0680,  0.0232,  ...,  0.1364, -0.0319, -0.0557],\n",
       "         [ 0.1074,  0.0680,  0.0232,  ...,  0.1364, -0.0319, -0.0557],\n",
       "         [ 0.1074,  0.0680,  0.0232,  ...,  0.1364, -0.0319, -0.0557]],\n",
       "\n",
       "        [[-0.0231,  0.0543, -0.0471,  ..., -0.1168, -0.0329, -0.0221],\n",
       "         [-0.0257, -0.3383, -0.0398,  ..., -0.2831,  0.1596, -0.1052],\n",
       "         [ 0.1653,  0.0165, -0.0106,  ..., -0.2187,  0.2102, -0.0407],\n",
       "         ...,\n",
       "         [ 0.0328, -0.0216,  0.0735,  ..., -0.1318, -0.0123,  0.0299],\n",
       "         [ 0.0328, -0.0216,  0.0735,  ..., -0.1318, -0.0123,  0.0299],\n",
       "         [ 0.0328, -0.0216,  0.0735,  ..., -0.1318, -0.0123,  0.0299]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0392,  0.0620, -0.0396,  ..., -0.0945, -0.0609, -0.0203],\n",
       "         [ 0.0842, -0.4585, -0.2374,  ..., -0.2029,  0.0597,  0.1723],\n",
       "         [-0.0554,  0.1015,  0.0998,  ..., -0.4048, -0.0006,  0.2756],\n",
       "         ...,\n",
       "         [ 0.1201, -0.0252,  0.0802,  ...,  0.0705,  0.0877, -0.0928],\n",
       "         [ 0.1201, -0.0252,  0.0802,  ...,  0.0705,  0.0877, -0.0928],\n",
       "         [ 0.1201, -0.0252,  0.0802,  ...,  0.0705,  0.0877, -0.0928]],\n",
       "\n",
       "        [[-0.0244,  0.0636, -0.0208,  ..., -0.1070, -0.0254, -0.0335],\n",
       "         [-0.0851, -0.2687, -0.1086,  ..., -0.4214,  0.1786,  0.0464],\n",
       "         [ 0.0725, -0.1549,  0.1507,  ..., -0.1649,  0.1020,  0.0540],\n",
       "         ...,\n",
       "         [ 0.0575, -0.0138,  0.0420,  ..., -0.0333, -0.0209, -0.0271],\n",
       "         [ 0.0575, -0.0138,  0.0420,  ..., -0.0333, -0.0209, -0.0271],\n",
       "         [ 0.0575, -0.0138,  0.0420,  ..., -0.0333, -0.0209, -0.0271]],\n",
       "\n",
       "        [[-0.0564,  0.0435, -0.0183,  ..., -0.1650, -0.0948, -0.0094],\n",
       "         [ 0.0018,  0.1074, -0.0314,  ..., -0.0649, -0.0318, -0.1093],\n",
       "         [-0.0212, -0.0546, -0.1059,  ...,  0.0622, -0.2630,  0.0517],\n",
       "         ...,\n",
       "         [ 0.0290, -0.1055,  0.0856,  ..., -0.0795, -0.1458,  0.0297],\n",
       "         [ 0.0290, -0.1055,  0.0856,  ..., -0.0795, -0.1458,  0.0297],\n",
       "         [ 0.0290, -0.1055,  0.0856,  ..., -0.0795, -0.1458,  0.0297]]]), pooler_output=tensor([[ 0.0666,  0.1696,  0.4001,  ..., -0.3890, -0.2504,  0.2261],\n",
       "        [ 0.0468,  0.1830,  0.4022,  ..., -0.3785, -0.2835,  0.2257],\n",
       "        [ 0.0527,  0.1843,  0.3904,  ..., -0.3909, -0.2722,  0.2080],\n",
       "        ...,\n",
       "        [ 0.0461,  0.2013,  0.4127,  ..., -0.3796, -0.2511,  0.2183],\n",
       "        [ 0.0491,  0.1889,  0.3933,  ..., -0.3800, -0.2707,  0.2113],\n",
       "        [ 0.0698,  0.1962,  0.3997,  ..., -0.3982, -0.2557,  0.2252]]), hidden_states=(tensor([[[ 0.1664, -0.0541, -0.0014,  ..., -0.0811,  0.0794,  0.0155],\n",
       "         [ 0.5156, -0.2726,  0.0586,  ..., -0.2067, -0.1151,  0.5307],\n",
       "         [ 0.1036, -0.6125, -0.0224,  ..., -0.1052, -0.0580,  0.1356],\n",
       "         ...,\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947]],\n",
       "\n",
       "        [[ 0.1664, -0.0541, -0.0014,  ..., -0.0811,  0.0794,  0.0155],\n",
       "         [-0.2018, -0.0972, -0.3327,  ..., -0.4031,  0.2407,  0.0722],\n",
       "         [ 0.3036, -0.2536,  0.2910,  ...,  0.0482, -0.0449, -0.0010],\n",
       "         ...,\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947]],\n",
       "\n",
       "        [[ 0.1664, -0.0541, -0.0014,  ..., -0.0811,  0.0794,  0.0155],\n",
       "         [-0.3628, -0.1290,  0.1091,  ..., -0.0186, -0.4578,  0.0093],\n",
       "         [-0.0266,  0.3270, -0.2449,  ...,  0.1357, -0.0839,  0.1221],\n",
       "         ...,\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.1664, -0.0541, -0.0014,  ..., -0.0811,  0.0794,  0.0155],\n",
       "         [ 0.5156, -0.2726,  0.0586,  ..., -0.2067, -0.1151,  0.5307],\n",
       "         [ 0.2069, -0.2458,  0.3092,  ...,  0.0062, -0.4684,  0.1875],\n",
       "         ...,\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947]],\n",
       "\n",
       "        [[ 0.1664, -0.0541, -0.0014,  ..., -0.0811,  0.0794,  0.0155],\n",
       "         [ 0.5156, -0.2726,  0.0586,  ..., -0.2067, -0.1151,  0.5307],\n",
       "         [ 0.5425,  0.1412, -0.3080,  ..., -0.1417, -0.5764,  0.1147],\n",
       "         ...,\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947]],\n",
       "\n",
       "        [[ 0.1664, -0.0541, -0.0014,  ..., -0.0811,  0.0794,  0.0155],\n",
       "         [-0.3020, -0.4419,  0.4570,  ...,  0.1405, -0.3432,  0.0391],\n",
       "         [ 0.0778, -0.0085, -0.1923,  ...,  0.4366, -0.3112,  0.3649],\n",
       "         ...,\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
       "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947]]]), tensor([[[-0.0021,  0.0223,  0.0250,  ...,  0.0148,  0.0113, -0.1435],\n",
       "         [ 0.3674, -0.2405,  0.1080,  ..., -0.1749, -0.4507,  0.8114],\n",
       "         [ 0.0756, -0.5013,  0.1415,  ..., -0.0794, -0.1141,  0.4119],\n",
       "         ...,\n",
       "         [-0.4965,  0.1320, -0.0420,  ...,  0.0870, -0.1767,  0.0788],\n",
       "         [-0.4965,  0.1320, -0.0420,  ...,  0.0870, -0.1767,  0.0788],\n",
       "         [-0.4965,  0.1320, -0.0420,  ...,  0.0870, -0.1767,  0.0788]],\n",
       "\n",
       "        [[-0.0212,  0.0214,  0.0068,  ...,  0.0013,  0.0187, -0.1613],\n",
       "         [-0.0167, -0.1746, -0.5418,  ..., -0.5530,  0.4852,  0.1865],\n",
       "         [ 0.2562, -0.4654,  0.2716,  ..., -0.3140,  0.0955, -0.2462],\n",
       "         ...,\n",
       "         [-0.5692,  0.1784, -0.0433,  ...,  0.0292, -0.0948,  0.0497],\n",
       "         [-0.5692,  0.1784, -0.0433,  ...,  0.0292, -0.0948,  0.0497],\n",
       "         [-0.5692,  0.1784, -0.0433,  ...,  0.0292, -0.0948,  0.0497]],\n",
       "\n",
       "        [[-0.0190,  0.0303,  0.0357,  ..., -0.0162,  0.0240, -0.1396],\n",
       "         [-0.0696, -0.0159,  0.3252,  ...,  0.2062, -0.2294,  0.0046],\n",
       "         [ 0.4523,  0.6879, -0.1514,  ..., -0.0952, -0.2054,  0.2850],\n",
       "         ...,\n",
       "         [-0.5071,  0.2248, -0.0212,  ...,  0.0791, -0.0477, -0.0152],\n",
       "         [-0.5071,  0.2248, -0.0212,  ...,  0.0791, -0.0477, -0.0152],\n",
       "         [-0.5071,  0.2248, -0.0212,  ...,  0.0791, -0.0477, -0.0152]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0061,  0.0063,  0.0192,  ..., -0.0058,  0.0048, -0.1664],\n",
       "         [ 0.5807, -0.4380,  0.0813,  ..., -0.2443, -0.2909,  0.8604],\n",
       "         [ 0.3513, -0.2275,  0.4137,  ..., -0.1438, -0.3757,  0.2365],\n",
       "         ...,\n",
       "         [-0.5280,  0.1843, -0.0240,  ...,  0.0949, -0.0920,  0.0284],\n",
       "         [-0.5280,  0.1843, -0.0240,  ...,  0.0949, -0.0920,  0.0284],\n",
       "         [-0.5280,  0.1843, -0.0240,  ...,  0.0949, -0.0920,  0.0284]],\n",
       "\n",
       "        [[-0.0106,  0.0194,  0.0333,  ..., -0.0113,  0.0234, -0.1446],\n",
       "         [ 0.3396, -0.4172,  0.0989,  ..., -0.2445, -0.3750,  1.0545],\n",
       "         [ 0.3950,  0.1160, -0.5242,  ..., -0.1683, -0.7614,  0.2663],\n",
       "         ...,\n",
       "         [-0.5899,  0.1877, -0.0183,  ...,  0.0635, -0.0144,  0.0905],\n",
       "         [-0.5899,  0.1877, -0.0183,  ...,  0.0635, -0.0144,  0.0905],\n",
       "         [-0.5899,  0.1877, -0.0183,  ...,  0.0635, -0.0144,  0.0905]],\n",
       "\n",
       "        [[-0.0034,  0.0081,  0.0175,  ..., -0.0189,  0.0315, -0.1530],\n",
       "         [-0.3709, -0.7223,  0.5019,  ...,  0.5791, -0.1320,  0.1576],\n",
       "         [ 0.1445,  0.0942, -0.2150,  ...,  0.6420, -0.0073,  0.2144],\n",
       "         ...,\n",
       "         [-0.5092,  0.1245, -0.0710,  ...,  0.0747, -0.0756,  0.0028],\n",
       "         [-0.5092,  0.1245, -0.0710,  ...,  0.0747, -0.0756,  0.0028],\n",
       "         [-0.5092,  0.1245, -0.0710,  ...,  0.0747, -0.0756,  0.0028]]]), tensor([[[ 4.5911e-02,  3.2675e-02,  4.8968e-03,  ...,  8.8447e-03,\n",
       "          -1.0586e-02, -5.7684e-02],\n",
       "         [ 2.5449e-01, -2.4177e-01, -4.3279e-01,  ..., -5.0674e-01,\n",
       "          -7.4777e-02,  9.4369e-01],\n",
       "         [ 4.2188e-01, -6.2618e-01, -2.0552e-02,  ..., -6.4267e-02,\n",
       "          -2.5525e-01,  5.0185e-01],\n",
       "         ...,\n",
       "         [ 1.4798e-01,  2.7987e-01,  1.0220e-01,  ..., -1.3771e-02,\n",
       "           6.2186e-02,  4.3413e-01],\n",
       "         [ 1.4798e-01,  2.7987e-01,  1.0220e-01,  ..., -1.3771e-02,\n",
       "           6.2186e-02,  4.3413e-01],\n",
       "         [ 1.4798e-01,  2.7987e-01,  1.0220e-01,  ..., -1.3771e-02,\n",
       "           6.2186e-02,  4.3413e-01]],\n",
       "\n",
       "        [[ 4.4355e-02,  3.6100e-02, -1.5028e-02,  ...,  1.0025e-02,\n",
       "          -2.0614e-02, -6.4852e-02],\n",
       "         [-2.6800e-02, -3.1455e-01, -7.3519e-01,  ..., -6.4746e-01,\n",
       "           1.3659e-01,  1.6471e-01],\n",
       "         [ 5.5600e-02, -3.6347e-01, -2.1866e-02,  ..., -2.9608e-02,\n",
       "           1.1377e-01, -3.2246e-01],\n",
       "         ...,\n",
       "         [ 1.1722e-01,  8.5228e-02, -4.4852e-03,  ...,  1.8905e-02,\n",
       "           1.7914e-02,  1.3405e-01],\n",
       "         [ 1.1722e-01,  8.5228e-02, -4.4852e-03,  ...,  1.8905e-02,\n",
       "           1.7914e-02,  1.3405e-01],\n",
       "         [ 1.1722e-01,  8.5228e-02, -4.4852e-03,  ...,  1.8905e-02,\n",
       "           1.7914e-02,  1.3405e-01]],\n",
       "\n",
       "        [[ 4.4077e-02,  3.7152e-02,  4.3849e-05,  ...,  9.6793e-03,\n",
       "          -1.9362e-02, -5.7922e-02],\n",
       "         [-1.0165e-01, -2.3015e-01, -7.7343e-02,  ...,  3.1843e-01,\n",
       "          -9.3963e-02,  4.4966e-01],\n",
       "         [-5.1567e-02,  6.5942e-01,  5.7896e-02,  ...,  3.2310e-02,\n",
       "           3.1397e-02,  4.0267e-01],\n",
       "         ...,\n",
       "         [-9.4052e-02,  1.5155e-01, -1.0293e-02,  ..., -6.3631e-02,\n",
       "           2.4457e-02,  2.1321e-01],\n",
       "         [-9.4052e-02,  1.5155e-01, -1.0293e-02,  ..., -6.3631e-02,\n",
       "           2.4457e-02,  2.1321e-01],\n",
       "         [-9.4052e-02,  1.5155e-01, -1.0293e-02,  ..., -6.3631e-02,\n",
       "           2.4457e-02,  2.1321e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 5.0761e-02,  2.9118e-02, -1.0871e-02,  ...,  3.6389e-03,\n",
       "          -9.3875e-03, -6.8579e-02],\n",
       "         [ 5.9207e-01, -3.7041e-01, -5.7764e-01,  ..., -5.5867e-01,\n",
       "           2.2189e-01,  1.0026e+00],\n",
       "         [ 2.3024e-01, -3.2526e-01,  1.6150e-01,  ...,  3.2791e-02,\n",
       "          -4.2808e-02,  5.7973e-01],\n",
       "         ...,\n",
       "         [ 5.3539e-02,  1.1044e-01,  5.1804e-02,  ..., -9.9557e-02,\n",
       "          -6.9018e-02,  2.4788e-01],\n",
       "         [ 5.3539e-02,  1.1044e-01,  5.1804e-02,  ..., -9.9557e-02,\n",
       "          -6.9018e-02,  2.4788e-01],\n",
       "         [ 5.3539e-02,  1.1044e-01,  5.1804e-02,  ..., -9.9557e-02,\n",
       "          -6.9018e-02,  2.4788e-01]],\n",
       "\n",
       "        [[ 4.5106e-02,  3.0704e-02, -1.1161e-02,  ...,  6.1814e-03,\n",
       "          -2.3273e-02, -5.4218e-02],\n",
       "         [ 2.5042e-01, -6.2275e-01, -2.7230e-01,  ..., -4.5712e-01,\n",
       "           1.3377e-01,  1.2731e+00],\n",
       "         [ 3.8765e-01, -2.6656e-02, -3.9005e-01,  ..., -1.5611e-01,\n",
       "          -8.0210e-01,  2.5379e-01],\n",
       "         ...,\n",
       "         [ 7.8550e-02,  2.0410e-01, -1.9128e-01,  ..., -9.5792e-02,\n",
       "          -2.0659e-02,  5.0370e-01],\n",
       "         [ 7.8550e-02,  2.0410e-01, -1.9128e-01,  ..., -9.5792e-02,\n",
       "          -2.0659e-02,  5.0370e-01],\n",
       "         [ 7.8550e-02,  2.0410e-01, -1.9128e-01,  ..., -9.5792e-02,\n",
       "          -2.0659e-02,  5.0370e-01]],\n",
       "\n",
       "        [[ 6.8715e-02,  4.8036e-02, -8.2314e-03,  ...,  5.2057e-03,\n",
       "           6.6973e-03, -6.0337e-02],\n",
       "         [ 2.2464e-01, -5.4671e-01,  2.6890e-01,  ...,  4.4786e-01,\n",
       "           2.4392e-02,  5.0259e-01],\n",
       "         [ 7.8640e-02, -2.1434e-01, -8.0778e-01,  ...,  6.3800e-01,\n",
       "          -7.1178e-02,  5.5708e-01],\n",
       "         ...,\n",
       "         [ 1.6628e-01,  3.5264e-01,  5.6195e-02,  ..., -6.4886e-03,\n",
       "          -5.9427e-03,  2.5874e-01],\n",
       "         [ 1.6628e-01,  3.5264e-01,  5.6195e-02,  ..., -6.4886e-03,\n",
       "          -5.9427e-03,  2.5874e-01],\n",
       "         [ 1.6628e-01,  3.5264e-01,  5.6195e-02,  ..., -6.4886e-03,\n",
       "          -5.9427e-03,  2.5874e-01]]]), tensor([[[ 0.0344,  0.0575, -0.0206,  ...,  0.0100,  0.0590,  0.0450],\n",
       "         [ 0.1751, -0.1894,  0.0534,  ..., -0.6475, -0.3673,  0.5979],\n",
       "         [ 0.0416, -0.1319, -0.4072,  ..., -0.1733, -0.1475,  0.1485],\n",
       "         ...,\n",
       "         [-0.1146,  0.1816,  0.1256,  ..., -0.0315, -0.0835,  0.4621],\n",
       "         [-0.1146,  0.1816,  0.1256,  ..., -0.0315, -0.0835,  0.4621],\n",
       "         [-0.1146,  0.1816,  0.1256,  ..., -0.0315, -0.0835,  0.4621]],\n",
       "\n",
       "        [[ 0.0219,  0.0528, -0.0281,  ...,  0.0121,  0.0468,  0.0344],\n",
       "         [-0.1712, -0.4238, -0.3427,  ..., -0.6486,  0.0504,  0.1684],\n",
       "         [ 0.0649, -0.3576, -0.3410,  ..., -0.0707, -0.1429, -0.3165],\n",
       "         ...,\n",
       "         [ 0.0261, -0.0179, -0.0143,  ..., -0.0197, -0.2476,  0.1314],\n",
       "         [ 0.0261, -0.0179, -0.0143,  ..., -0.0197, -0.2476,  0.1314],\n",
       "         [ 0.0261, -0.0179, -0.0143,  ..., -0.0197, -0.2476,  0.1314]],\n",
       "\n",
       "        [[ 0.0191,  0.0529, -0.0239,  ...,  0.0099,  0.0459,  0.0395],\n",
       "         [-0.1898, -0.2634,  0.4059,  ...,  0.1212, -0.4795,  0.5098],\n",
       "         [-0.2606,  0.5224,  0.2728,  ..., -0.1284,  0.0964,  0.1722],\n",
       "         ...,\n",
       "         [-0.0640,  0.1494, -0.0539,  ..., -0.1012, -0.0907,  0.1916],\n",
       "         [-0.0640,  0.1494, -0.0539,  ..., -0.1012, -0.0907,  0.1916],\n",
       "         [-0.0640,  0.1494, -0.0539,  ..., -0.1012, -0.0907,  0.1916]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0301,  0.0534, -0.0248,  ...,  0.0070,  0.0467,  0.0350],\n",
       "         [ 0.5719, -0.5973, -0.3013,  ..., -0.5741, -0.0526,  0.7560],\n",
       "         [ 0.3941, -0.3213,  0.0555,  ..., -0.1456, -0.1549,  0.4475],\n",
       "         ...,\n",
       "         [-0.0206, -0.0359, -0.0532,  ..., -0.0889, -0.1635,  0.2651],\n",
       "         [-0.0206, -0.0359, -0.0532,  ..., -0.0889, -0.1635,  0.2651],\n",
       "         [-0.0206, -0.0359, -0.0532,  ..., -0.0889, -0.1635,  0.2651]],\n",
       "\n",
       "        [[ 0.0205,  0.0464, -0.0259,  ...,  0.0154,  0.0488,  0.0433],\n",
       "         [ 0.2445, -0.6764,  0.2289,  ..., -0.5431, -0.2433,  1.0210],\n",
       "         [ 0.3243,  0.1707, -0.4663,  ..., -0.2761, -0.2651,  0.0999],\n",
       "         ...,\n",
       "         [-0.0931,  0.0274, -0.1209,  ..., -0.1478, -0.1334,  0.6184],\n",
       "         [-0.0931,  0.0274, -0.1209,  ..., -0.1478, -0.1334,  0.6184],\n",
       "         [-0.0931,  0.0274, -0.1209,  ..., -0.1478, -0.1334,  0.6184]],\n",
       "\n",
       "        [[ 0.0405,  0.0603, -0.0272,  ...,  0.0073,  0.0603,  0.0328],\n",
       "         [ 0.3226, -0.2361,  0.0026,  ...,  0.4750, -0.1603,  0.3205],\n",
       "         [ 0.1422, -0.0754, -0.6083,  ...,  0.2409, -0.1247,  0.4554],\n",
       "         ...,\n",
       "         [ 0.0080,  0.1547,  0.0743,  ...,  0.0540, -0.2009,  0.2137],\n",
       "         [ 0.0080,  0.1547,  0.0743,  ...,  0.0540, -0.2009,  0.2137],\n",
       "         [ 0.0080,  0.1547,  0.0743,  ...,  0.0540, -0.2009,  0.2137]]]), tensor([[[ 3.5292e-02,  6.3392e-02, -1.2578e-03,  ...,  1.5162e-02,\n",
       "          -1.1832e-02, -4.7318e-02],\n",
       "         [-6.0325e-02, -4.5014e-01,  1.9074e-01,  ..., -3.2129e-01,\n",
       "          -5.9489e-01,  2.2003e-01],\n",
       "         [-2.9006e-01,  7.5634e-02, -3.5841e-01,  ..., -9.4596e-02,\n",
       "          -8.2249e-02,  1.8560e-01],\n",
       "         ...,\n",
       "         [-2.4859e-01,  2.9264e-01,  1.4341e-01,  ...,  1.3331e-01,\n",
       "           1.6530e-01,  1.4123e-01],\n",
       "         [-2.4859e-01,  2.9264e-01,  1.4341e-01,  ...,  1.3331e-01,\n",
       "           1.6530e-01,  1.4123e-01],\n",
       "         [-2.4859e-01,  2.9264e-01,  1.4341e-01,  ...,  1.3331e-01,\n",
       "           1.6530e-01,  1.4123e-01]],\n",
       "\n",
       "        [[ 2.5266e-02,  5.8366e-02, -5.3200e-03,  ...,  1.4076e-02,\n",
       "          -2.0127e-02, -4.4203e-02],\n",
       "         [-5.0788e-01, -5.6672e-01, -3.9687e-01,  ..., -3.0961e-01,\n",
       "           1.6569e-01,  1.7434e-01],\n",
       "         [-1.4464e-01, -2.6017e-01, -4.4205e-01,  ...,  1.5514e-01,\n",
       "          -1.1695e-01, -2.1344e-01],\n",
       "         ...,\n",
       "         [-1.1693e-01,  1.1622e-01,  6.7404e-02,  ...,  1.1330e-01,\n",
       "           6.0968e-02, -3.6087e-02],\n",
       "         [-1.1693e-01,  1.1622e-01,  6.7404e-02,  ...,  1.1330e-01,\n",
       "           6.0968e-02, -3.6087e-02],\n",
       "         [-1.1693e-01,  1.1622e-01,  6.7404e-02,  ...,  1.1330e-01,\n",
       "           6.0968e-02, -3.6087e-02]],\n",
       "\n",
       "        [[ 3.5703e-02,  5.3928e-02,  8.3753e-04,  ...,  1.0424e-02,\n",
       "          -1.6417e-02, -4.2159e-02],\n",
       "         [-3.7989e-01, -4.9463e-01,  3.8014e-01,  ...,  3.1367e-01,\n",
       "          -3.2673e-01,  3.2737e-02],\n",
       "         [-8.8103e-02,  3.2336e-01,  3.0392e-01,  ..., -1.0805e-01,\n",
       "          -9.7060e-02,  6.7242e-02],\n",
       "         ...,\n",
       "         [ 4.6187e-03,  2.6533e-01,  7.8586e-02,  ..., -6.4179e-02,\n",
       "           9.9396e-02,  4.5886e-02],\n",
       "         [ 4.6187e-03,  2.6533e-01,  7.8586e-02,  ..., -6.4179e-02,\n",
       "           9.9396e-02,  4.5886e-02],\n",
       "         [ 4.6187e-03,  2.6533e-01,  7.8586e-02,  ..., -6.4179e-02,\n",
       "           9.9396e-02,  4.5886e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.8233e-02,  4.1663e-02,  5.2845e-03,  ...,  4.4677e-03,\n",
       "          -2.9804e-02, -6.4455e-02],\n",
       "         [-2.7716e-01, -6.0203e-01, -2.3789e-01,  ..., -1.9003e-01,\n",
       "          -3.4436e-01,  2.7766e-01],\n",
       "         [-3.7418e-02, -2.8927e-01, -3.1292e-01,  ..., -1.0216e-01,\n",
       "          -1.7799e-01,  2.3112e-01],\n",
       "         ...,\n",
       "         [-1.3584e-01,  2.0795e-01, -1.3450e-02,  ..., -2.7503e-02,\n",
       "           3.4309e-02,  1.9261e-01],\n",
       "         [-1.3584e-01,  2.0795e-01, -1.3450e-02,  ..., -2.7503e-02,\n",
       "           3.4309e-02,  1.9261e-01],\n",
       "         [-1.3584e-01,  2.0795e-01, -1.3450e-02,  ..., -2.7503e-02,\n",
       "           3.4309e-02,  1.9261e-01]],\n",
       "\n",
       "        [[ 2.5702e-02,  5.4702e-02, -6.0133e-03,  ...,  2.1314e-02,\n",
       "          -2.0916e-02, -3.9199e-02],\n",
       "         [-2.3198e-01, -9.6732e-01,  3.0005e-01,  ..., -2.8155e-01,\n",
       "          -2.9585e-01,  7.2633e-01],\n",
       "         [-1.8429e-01,  2.0099e-01,  1.3784e-01,  ..., -3.8431e-01,\n",
       "          -2.3590e-01, -4.4482e-01],\n",
       "         ...,\n",
       "         [-1.1306e-01,  1.4094e-01,  2.4871e-02,  ..., -8.8014e-02,\n",
       "           1.5176e-01,  1.5396e-01],\n",
       "         [-1.1306e-01,  1.4094e-01,  2.4871e-02,  ..., -8.8014e-02,\n",
       "           1.5176e-01,  1.5396e-01],\n",
       "         [-1.1306e-01,  1.4094e-01,  2.4871e-02,  ..., -8.8014e-02,\n",
       "           1.5176e-01,  1.5396e-01]],\n",
       "\n",
       "        [[ 6.6074e-02,  3.5931e-02,  9.7474e-03,  ..., -3.9749e-03,\n",
       "          -1.9572e-02, -6.6140e-02],\n",
       "         [-1.4777e-01, -1.5489e-01,  7.0063e-03,  ...,  5.5735e-01,\n",
       "          -2.1172e-01,  5.5895e-02],\n",
       "         [-4.5122e-01,  9.5535e-02, -4.7024e-01,  ...,  3.7079e-01,\n",
       "          -1.5119e-01,  3.1983e-01],\n",
       "         ...,\n",
       "         [-1.6224e-01,  2.5507e-01,  8.7315e-02,  ...,  1.7612e-01,\n",
       "           1.8819e-01, -1.2289e-01],\n",
       "         [-1.6224e-01,  2.5507e-01,  8.7315e-02,  ...,  1.7612e-01,\n",
       "           1.8819e-01, -1.2289e-01],\n",
       "         [-1.6224e-01,  2.5507e-01,  8.7315e-02,  ...,  1.7612e-01,\n",
       "           1.8819e-01, -1.2289e-01]]]), tensor([[[ 0.0435,  0.0873,  0.0028,  ..., -0.0208,  0.0392,  0.0139],\n",
       "         [-0.2656, -0.4473,  0.3054,  ..., -0.4932, -0.4771, -0.1276],\n",
       "         [-0.0485, -0.1334, -0.0581,  ..., -0.1466, -0.2211,  0.1065],\n",
       "         ...,\n",
       "         [-0.2977,  0.1730,  0.2044,  ...,  0.1590,  0.6505, -0.0745],\n",
       "         [-0.2977,  0.1730,  0.2044,  ...,  0.1590,  0.6505, -0.0745],\n",
       "         [-0.2977,  0.1730,  0.2044,  ...,  0.1590,  0.6505, -0.0745]],\n",
       "\n",
       "        [[ 0.0431,  0.0824,  0.0150,  ..., -0.0139,  0.0267,  0.0075],\n",
       "         [-0.1800, -0.5013, -0.0246,  ..., -0.3064, -0.1440, -0.1323],\n",
       "         [ 0.4645, -0.1833, -0.4385,  ...,  0.3102, -0.1094, -0.1747],\n",
       "         ...,\n",
       "         [-0.2030,  0.0432,  0.0708,  ...,  0.1623,  0.5265, -0.0887],\n",
       "         [-0.2030,  0.0432,  0.0708,  ...,  0.1623,  0.5265, -0.0887],\n",
       "         [-0.2030,  0.0432,  0.0708,  ...,  0.1623,  0.5265, -0.0887]],\n",
       "\n",
       "        [[ 0.0477,  0.0373, -0.0198,  ..., -0.0221,  0.0362, -0.0139],\n",
       "         [-0.1831, -0.5342,  0.5613,  ..., -0.1185, -0.3574, -0.4566],\n",
       "         [-0.0353,  0.4854, -0.1565,  ..., -0.3083,  0.1891, -0.3042],\n",
       "         ...,\n",
       "         [-0.1972,  0.1669,  0.0761,  ..., -0.0496,  0.3830, -0.0324],\n",
       "         [-0.1972,  0.1669,  0.0761,  ..., -0.0496,  0.3830, -0.0324],\n",
       "         [-0.1972,  0.1669,  0.0761,  ..., -0.0496,  0.3830, -0.0324]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0552,  0.0533,  0.0048,  ..., -0.0225,  0.0476, -0.0270],\n",
       "         [-0.2705, -0.5786, -0.4346,  ..., -0.2492,  0.1255,  0.0427],\n",
       "         [-0.0050, -0.2030,  0.0258,  ..., -0.0563, -0.1971,  0.2719],\n",
       "         ...,\n",
       "         [-0.0587,  0.1599,  0.0348,  ...,  0.0016,  0.3047,  0.0619],\n",
       "         [-0.0587,  0.1599,  0.0348,  ...,  0.0016,  0.3047,  0.0619],\n",
       "         [-0.0587,  0.1599,  0.0348,  ...,  0.0016,  0.3047,  0.0619]],\n",
       "\n",
       "        [[ 0.0324,  0.0715,  0.0101,  ..., -0.0130,  0.0363,  0.0119],\n",
       "         [-0.3964, -0.9201,  0.0571,  ..., -0.4030, -0.2452, -0.1459],\n",
       "         [ 0.1679,  0.3123,  0.6037,  ..., -0.5762, -0.3470, -0.2166],\n",
       "         ...,\n",
       "         [-0.2950,  0.1047,  0.2323,  ...,  0.0406,  0.4652, -0.1098],\n",
       "         [-0.2950,  0.1047,  0.2323,  ...,  0.0406,  0.4652, -0.1098],\n",
       "         [-0.2950,  0.1047,  0.2323,  ...,  0.0406,  0.4652, -0.1098]],\n",
       "\n",
       "        [[ 0.0577,  0.0604,  0.0184,  ..., -0.0496,  0.0282, -0.0305],\n",
       "         [-0.0969, -0.1244, -0.1519,  ...,  0.3477, -0.1649, -0.2953],\n",
       "         [-0.1272, -0.0574,  0.0119,  ...,  0.1787, -0.2919,  0.1641],\n",
       "         ...,\n",
       "         [-0.3713,  0.0699,  0.2781,  ...,  0.2484,  0.6153, -0.1957],\n",
       "         [-0.3713,  0.0699,  0.2781,  ...,  0.2484,  0.6153, -0.1957],\n",
       "         [-0.3713,  0.0699,  0.2781,  ...,  0.2484,  0.6153, -0.1957]]]), tensor([[[ 9.4695e-02,  6.5199e-02,  1.1093e-01,  ..., -1.0125e-05,\n",
       "          -2.7526e-02, -4.0357e-02],\n",
       "         [-2.4771e-01, -7.8894e-01,  4.6410e-01,  ..., -6.1353e-01,\n",
       "          -4.8665e-01, -1.4618e-01],\n",
       "         [ 6.7464e-02, -1.1916e-01, -1.1079e-01,  ..., -2.7534e-01,\n",
       "          -2.1392e-03, -1.0964e-01],\n",
       "         ...,\n",
       "         [ 1.0249e-01,  1.6367e-01,  5.7324e-01,  ..., -8.2005e-02,\n",
       "           3.5005e-01, -1.3313e-01],\n",
       "         [ 1.0249e-01,  1.6367e-01,  5.7324e-01,  ..., -8.2005e-02,\n",
       "           3.5005e-01, -1.3313e-01],\n",
       "         [ 1.0249e-01,  1.6367e-01,  5.7324e-01,  ..., -8.2005e-02,\n",
       "           3.5005e-01, -1.3313e-01]],\n",
       "\n",
       "        [[ 9.7410e-02,  5.6607e-02,  1.1501e-01,  ...,  1.0336e-02,\n",
       "          -2.5084e-02, -2.6145e-02],\n",
       "         [-5.7741e-02, -4.0182e-01, -1.4311e-01,  ..., -5.1366e-01,\n",
       "          -1.9279e-01, -1.0496e-01],\n",
       "         [ 1.9037e-02,  6.1563e-02, -5.6408e-01,  ..., -1.0459e-01,\n",
       "           5.3192e-02, -4.3460e-01],\n",
       "         ...,\n",
       "         [ 2.2518e-01,  3.6211e-01,  3.0093e-01,  ...,  1.5512e-02,\n",
       "           3.1035e-01, -1.0751e-01],\n",
       "         [ 2.2518e-01,  3.6211e-01,  3.0093e-01,  ...,  1.5512e-02,\n",
       "           3.1035e-01, -1.0751e-01],\n",
       "         [ 2.2518e-01,  3.6211e-01,  3.0093e-01,  ...,  1.5512e-02,\n",
       "           3.1035e-01, -1.0751e-01]],\n",
       "\n",
       "        [[ 8.7979e-02,  2.9005e-02,  9.2416e-02,  ..., -3.2843e-03,\n",
       "          -1.0169e-02, -4.6827e-02],\n",
       "         [-7.7101e-01, -7.7296e-01,  7.9107e-01,  ..., -7.0164e-01,\n",
       "          -1.1068e-02, -3.9347e-01],\n",
       "         [-4.8004e-01,  4.0436e-01, -1.4785e-01,  ..., -7.1893e-01,\n",
       "           3.7638e-02, -3.8533e-01],\n",
       "         ...,\n",
       "         [ 6.1234e-02,  2.5688e-01,  4.4937e-01,  ..., -8.1910e-02,\n",
       "           1.8730e-01, -1.3412e-01],\n",
       "         [ 6.1234e-02,  2.5688e-01,  4.4937e-01,  ..., -8.1910e-02,\n",
       "           1.8730e-01, -1.3412e-01],\n",
       "         [ 6.1234e-02,  2.5688e-01,  4.4937e-01,  ..., -8.1910e-02,\n",
       "           1.8730e-01, -1.3412e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.0920e-01,  4.7616e-02,  1.2032e-01,  ..., -5.9465e-04,\n",
       "          -1.7457e-02, -4.7251e-02],\n",
       "         [ 6.9028e-02, -8.9263e-01, -8.0026e-02,  ..., -3.4438e-01,\n",
       "           3.8520e-01, -7.7224e-02],\n",
       "         [ 2.6229e-02,  1.1879e-01, -3.8658e-02,  ..., -2.6034e-01,\n",
       "          -4.5158e-01,  3.0486e-01],\n",
       "         ...,\n",
       "         [ 2.9971e-01,  1.9323e-01,  3.7112e-01,  ..., -1.8138e-01,\n",
       "           2.1068e-01, -1.2201e-01],\n",
       "         [ 2.9971e-01,  1.9323e-01,  3.7112e-01,  ..., -1.8138e-01,\n",
       "           2.1068e-01, -1.2201e-01],\n",
       "         [ 2.9971e-01,  1.9323e-01,  3.7112e-01,  ..., -1.8138e-01,\n",
       "           2.1068e-01, -1.2201e-01]],\n",
       "\n",
       "        [[ 9.4720e-02,  5.5403e-02,  1.1571e-01,  ...,  8.7928e-03,\n",
       "          -1.6464e-02, -3.6012e-02],\n",
       "         [ 1.1134e-01, -1.2392e+00,  1.9389e-01,  ..., -6.1668e-01,\n",
       "          -3.0997e-01, -1.5311e-01],\n",
       "         [ 6.8971e-02,  1.7536e-01,  4.5045e-01,  ..., -4.7873e-01,\n",
       "          -1.6168e-01, -7.0332e-02],\n",
       "         ...,\n",
       "         [ 8.1178e-02, -6.6798e-02,  8.8559e-01,  ..., -2.2432e-01,\n",
       "           3.1821e-01, -1.0975e-01],\n",
       "         [ 8.1178e-02, -6.6798e-02,  8.8559e-01,  ..., -2.2432e-01,\n",
       "           3.1821e-01, -1.0975e-01],\n",
       "         [ 8.1178e-02, -6.6798e-02,  8.8559e-01,  ..., -2.2432e-01,\n",
       "           3.1821e-01, -1.0975e-01]],\n",
       "\n",
       "        [[ 9.7372e-02,  2.2483e-02,  9.9150e-02,  ..., -3.8553e-02,\n",
       "          -1.4111e-02, -6.7217e-02],\n",
       "         [-5.9146e-01, -1.5474e-01, -1.1080e-01,  ..., -8.4139e-02,\n",
       "           3.7301e-01, -3.7278e-01],\n",
       "         [-4.7972e-01, -1.5713e-01, -4.1057e-01,  ...,  3.4296e-02,\n",
       "           1.8511e-01, -2.7555e-01],\n",
       "         ...,\n",
       "         [ 1.4195e-02,  8.6450e-02,  6.8586e-01,  ...,  1.2621e-02,\n",
       "           3.4639e-01, -7.1765e-02],\n",
       "         [ 1.4195e-02,  8.6450e-02,  6.8586e-01,  ...,  1.2621e-02,\n",
       "           3.4639e-01, -7.1765e-02],\n",
       "         [ 1.4195e-02,  8.6450e-02,  6.8586e-01,  ...,  1.2621e-02,\n",
       "           3.4639e-01, -7.1765e-02]]]), tensor([[[-2.3000e-03,  5.5636e-02,  6.0745e-02,  ...,  1.0825e-01,\n",
       "          -3.2548e-02, -1.2024e-02],\n",
       "         [-1.9281e-01, -1.0650e+00,  3.0208e-01,  ..., -5.9243e-01,\n",
       "          -7.3140e-02, -4.9196e-02],\n",
       "         [ 2.1635e-01,  2.3324e-01, -7.7927e-02,  ..., -2.0851e-01,\n",
       "           2.4931e-01,  1.0408e-01],\n",
       "         ...,\n",
       "         [ 1.1792e-01, -2.3070e-01,  7.1510e-01,  ...,  1.3807e-01,\n",
       "           4.4962e-01, -6.1607e-02],\n",
       "         [ 1.1792e-01, -2.3070e-01,  7.1510e-01,  ...,  1.3807e-01,\n",
       "           4.4962e-01, -6.1607e-02],\n",
       "         [ 1.1792e-01, -2.3070e-01,  7.1510e-01,  ...,  1.3807e-01,\n",
       "           4.4962e-01, -6.1607e-02]],\n",
       "\n",
       "        [[ 1.0582e-04,  5.1339e-02,  5.7198e-02,  ...,  1.2081e-01,\n",
       "          -2.4907e-02, -2.0601e-02],\n",
       "         [-3.5245e-01, -8.1275e-01, -2.2551e-02,  ..., -4.4368e-01,\n",
       "           3.4894e-01, -1.8210e-01],\n",
       "         [-8.6225e-02,  2.9725e-02, -1.9317e-01,  ..., -7.0056e-02,\n",
       "           8.2601e-02, -5.3318e-01],\n",
       "         ...,\n",
       "         [ 2.4678e-01,  1.4625e-01,  5.7337e-01,  ...,  2.2850e-01,\n",
       "           6.7966e-01, -2.6332e-01],\n",
       "         [ 2.4678e-01,  1.4625e-01,  5.7337e-01,  ...,  2.2850e-01,\n",
       "           6.7966e-01, -2.6332e-01],\n",
       "         [ 2.4678e-01,  1.4625e-01,  5.7337e-01,  ...,  2.2850e-01,\n",
       "           6.7966e-01, -2.6332e-01]],\n",
       "\n",
       "        [[ 1.6643e-02,  4.9670e-02,  4.2461e-02,  ...,  1.1798e-01,\n",
       "          -1.4307e-02, -3.0104e-02],\n",
       "         [-4.6929e-01, -8.4940e-01,  4.0673e-01,  ..., -6.2514e-01,\n",
       "           4.4677e-02, -6.5204e-01],\n",
       "         [-3.9432e-01,  5.0794e-01, -2.4837e-01,  ..., -6.2485e-01,\n",
       "          -1.7021e-02, -6.9091e-01],\n",
       "         ...,\n",
       "         [-1.3072e-01, -1.1888e-01,  4.7091e-01,  ..., -1.1813e-01,\n",
       "           4.1992e-01, -2.2218e-01],\n",
       "         [-1.3072e-01, -1.1888e-01,  4.7091e-01,  ..., -1.1813e-01,\n",
       "           4.1992e-01, -2.2218e-01],\n",
       "         [-1.3072e-01, -1.1888e-01,  4.7091e-01,  ..., -1.1813e-01,\n",
       "           4.1992e-01, -2.2218e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.8159e-02,  5.5834e-02,  5.0741e-02,  ...,  1.1930e-01,\n",
       "          -4.4095e-03, -2.9923e-02],\n",
       "         [ 1.1514e-01, -1.0056e+00, -4.7845e-02,  ..., -3.4876e-01,\n",
       "           7.4991e-01,  3.2718e-03],\n",
       "         [ 3.1941e-02,  2.9262e-01,  2.3194e-01,  ..., -3.7299e-02,\n",
       "          -3.1159e-01,  9.1856e-02],\n",
       "         ...,\n",
       "         [ 4.0280e-02, -2.7458e-01,  5.0487e-01,  ...,  9.0598e-03,\n",
       "           8.2339e-01, -2.2213e-01],\n",
       "         [ 4.0280e-02, -2.7458e-01,  5.0487e-01,  ...,  9.0598e-03,\n",
       "           8.2339e-01, -2.2213e-01],\n",
       "         [ 4.0280e-02, -2.7458e-01,  5.0487e-01,  ...,  9.0598e-03,\n",
       "           8.2339e-01, -2.2213e-01]],\n",
       "\n",
       "        [[ 5.8657e-03,  6.6921e-02,  6.0410e-02,  ...,  1.2432e-01,\n",
       "          -3.1425e-02, -6.9922e-03],\n",
       "         [ 1.0020e-01, -1.0518e+00,  6.4323e-02,  ..., -6.4102e-01,\n",
       "          -6.1841e-02, -2.5900e-01],\n",
       "         [-2.1493e-01,  2.4863e-01,  4.9282e-01,  ..., -7.0856e-01,\n",
       "           2.1243e-01, -1.6098e-02],\n",
       "         ...,\n",
       "         [ 1.1537e-01, -2.7708e-01,  5.4349e-01,  ..., -3.6817e-02,\n",
       "           3.1219e-01, -9.4903e-02],\n",
       "         [ 1.1537e-01, -2.7708e-01,  5.4349e-01,  ..., -3.6817e-02,\n",
       "           3.1219e-01, -9.4903e-02],\n",
       "         [ 1.1537e-01, -2.7708e-01,  5.4349e-01,  ..., -3.6817e-02,\n",
       "           3.1219e-01, -9.4903e-02]],\n",
       "\n",
       "        [[ 2.5304e-02,  3.2948e-02,  8.5973e-02,  ...,  9.4856e-02,\n",
       "          -1.0702e-02, -5.8529e-02],\n",
       "         [-2.4700e-01, -3.6144e-01, -1.5382e-01,  ...,  2.4195e-02,\n",
       "           4.7173e-01, -5.6334e-01],\n",
       "         [-5.0028e-01, -6.0011e-02, -3.7537e-01,  ...,  1.4574e-01,\n",
       "           2.9596e-01, -8.2417e-01],\n",
       "         ...,\n",
       "         [-7.8380e-02, -3.5110e-01,  7.8008e-01,  ...,  2.0299e-01,\n",
       "           5.5385e-01, -7.6892e-02],\n",
       "         [-7.8380e-02, -3.5110e-01,  7.8008e-01,  ...,  2.0299e-01,\n",
       "           5.5385e-01, -7.6892e-02],\n",
       "         [-7.8380e-02, -3.5110e-01,  7.8008e-01,  ...,  2.0299e-01,\n",
       "           5.5385e-01, -7.6892e-02]]]), tensor([[[-0.0846,  0.1160,  0.0158,  ...,  0.1002,  0.0244, -0.0587],\n",
       "         [-0.4676, -1.1856,  0.2270,  ..., -0.6704, -0.1164,  0.2295],\n",
       "         [ 0.0794,  0.0805, -0.1329,  ..., -0.1551,  0.3307, -0.1469],\n",
       "         ...,\n",
       "         [-0.3886, -0.3782,  0.8123,  ...,  0.4733,  0.3962, -0.0123],\n",
       "         [-0.3886, -0.3782,  0.8123,  ...,  0.4733,  0.3962, -0.0123],\n",
       "         [-0.3886, -0.3782,  0.8123,  ...,  0.4733,  0.3962, -0.0123]],\n",
       "\n",
       "        [[-0.0767,  0.1275,  0.0106,  ...,  0.1085,  0.0148, -0.0480],\n",
       "         [-0.2600, -0.7553,  0.3789,  ..., -0.3171,  0.2159, -0.4018],\n",
       "         [ 0.0239, -0.0051,  0.1764,  ..., -0.1652,  0.4891, -0.7423],\n",
       "         ...,\n",
       "         [-0.3101, -0.2410,  0.7329,  ...,  0.5553,  0.2929, -0.2889],\n",
       "         [-0.3101, -0.2410,  0.7329,  ...,  0.5553,  0.2929, -0.2889],\n",
       "         [-0.3101, -0.2410,  0.7329,  ...,  0.5553,  0.2929, -0.2889]],\n",
       "\n",
       "        [[-0.0743,  0.1235, -0.0015,  ...,  0.0932,  0.0510, -0.0629],\n",
       "         [-0.5382, -1.0940,  0.4561,  ..., -0.5654,  0.1637, -0.7480],\n",
       "         [-0.0635,  0.4938, -0.4470,  ..., -0.5261,  0.0929, -0.6933],\n",
       "         ...,\n",
       "         [-0.3669, -0.4020,  0.6605,  ..., -0.0111,  0.2922, -0.3233],\n",
       "         [-0.3669, -0.4020,  0.6605,  ..., -0.0111,  0.2922, -0.3233],\n",
       "         [-0.3669, -0.4020,  0.6605,  ..., -0.0111,  0.2922, -0.3233]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0848,  0.1166,  0.0185,  ...,  0.0960,  0.0486, -0.0418],\n",
       "         [ 0.3563, -1.1809, -0.4957,  ..., -0.0260,  0.4634,  0.2523],\n",
       "         [ 0.0660,  0.2808,  0.0072,  ..., -0.0322, -0.2467, -0.1913],\n",
       "         ...,\n",
       "         [-0.2151, -0.6141,  0.4734,  ...,  0.1682,  0.7962, -0.3607],\n",
       "         [-0.2151, -0.6141,  0.4734,  ...,  0.1682,  0.7962, -0.3607],\n",
       "         [-0.2151, -0.6141,  0.4734,  ...,  0.1682,  0.7962, -0.3607]],\n",
       "\n",
       "        [[-0.0917,  0.1415,  0.0237,  ...,  0.1167,  0.0314, -0.0374],\n",
       "         [-0.5384, -1.0208, -0.2202,  ..., -0.7744,  0.1317, -0.0755],\n",
       "         [-0.0037,  0.1928, -0.0191,  ..., -0.6191,  0.3788,  0.0133],\n",
       "         ...,\n",
       "         [-0.5011, -0.4863,  0.7471,  ...,  0.3737,  0.2067, -0.1008],\n",
       "         [-0.5011, -0.4863,  0.7471,  ...,  0.3737,  0.2067, -0.1008],\n",
       "         [-0.5011, -0.4863,  0.7471,  ...,  0.3737,  0.2067, -0.1008]],\n",
       "\n",
       "        [[-0.0643,  0.1147,  0.0207,  ...,  0.0812,  0.0471, -0.1026],\n",
       "         [-0.2689, -0.4605,  0.2207,  ...,  0.0035,  0.3190, -0.3288],\n",
       "         [-0.2537, -0.0727, -0.0947,  ...,  0.0810, -0.0393, -0.4259],\n",
       "         ...,\n",
       "         [-0.5030, -0.5805,  0.7810,  ...,  0.6144,  0.2827, -0.1488],\n",
       "         [-0.5030, -0.5805,  0.7810,  ...,  0.6144,  0.2827, -0.1488],\n",
       "         [-0.5030, -0.5805,  0.7810,  ...,  0.6144,  0.2827, -0.1488]]]), tensor([[[-0.0850,  0.0394, -0.1296,  ...,  0.1252,  0.0698, -0.0389],\n",
       "         [ 0.0019, -1.0732,  0.0107,  ..., -0.8162,  0.1067,  0.1248],\n",
       "         [ 0.2590, -0.0057, -0.4139,  ..., -0.0778,  0.3163, -0.2182],\n",
       "         ...,\n",
       "         [-0.0024, -0.2387,  1.0034,  ...,  0.4043,  0.5806,  0.1949],\n",
       "         [-0.0024, -0.2387,  1.0034,  ...,  0.4043,  0.5806,  0.1949],\n",
       "         [-0.0024, -0.2387,  1.0034,  ...,  0.4043,  0.5806,  0.1949]],\n",
       "\n",
       "        [[-0.0904,  0.0437, -0.1282,  ...,  0.1269,  0.0600, -0.0249],\n",
       "         [-0.0133, -1.1153, -0.0525,  ..., -0.2618,  0.3393, -0.3177],\n",
       "         [ 0.1244, -0.0276, -0.1246,  ...,  0.0257,  0.3400, -0.7801],\n",
       "         ...,\n",
       "         [-0.0947, -0.0434,  0.7900,  ...,  0.5360,  0.4437,  0.0808],\n",
       "         [-0.0947, -0.0434,  0.7900,  ...,  0.5360,  0.4437,  0.0808],\n",
       "         [-0.0947, -0.0434,  0.7900,  ...,  0.5360,  0.4437,  0.0808]],\n",
       "\n",
       "        [[-0.0868,  0.0497, -0.1281,  ...,  0.1222,  0.0759, -0.0423],\n",
       "         [-0.2168, -0.9780, -0.3653,  ..., -0.6801,  0.0344, -0.2570],\n",
       "         [ 0.1626,  0.3781, -0.7953,  ..., -0.5677,  0.1784, -0.1989],\n",
       "         ...,\n",
       "         [ 0.0377, -0.1591,  0.7334,  ..., -0.1230,  0.4404, -0.2479],\n",
       "         [ 0.0377, -0.1591,  0.7334,  ..., -0.1230,  0.4404, -0.2479],\n",
       "         [ 0.0377, -0.1591,  0.7334,  ..., -0.1230,  0.4404, -0.2479]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0831,  0.0445, -0.1055,  ...,  0.1191,  0.0888, -0.0304],\n",
       "         [ 0.0236, -1.1920, -0.5445,  ..., -0.0369,  0.2642,  0.7335],\n",
       "         [ 0.0527,  0.3627,  0.1646,  ..., -0.1397, -0.0711,  0.1263],\n",
       "         ...,\n",
       "         [ 0.2451, -0.3721,  0.3920,  ...,  0.0276,  0.7464, -0.2929],\n",
       "         [ 0.2451, -0.3721,  0.3920,  ...,  0.0276,  0.7464, -0.2929],\n",
       "         [ 0.2451, -0.3721,  0.3920,  ...,  0.0276,  0.7464, -0.2929]],\n",
       "\n",
       "        [[-0.0862,  0.0474, -0.1190,  ...,  0.1246,  0.0543, -0.0297],\n",
       "         [ 0.0321, -0.9588, -0.0915,  ..., -0.7714,  0.1223, -0.1416],\n",
       "         [ 0.2847,  0.1832,  0.0953,  ..., -0.4939,  0.3799,  0.0395],\n",
       "         ...,\n",
       "         [-0.1082, -0.2822,  0.7219,  ...,  0.2569,  0.5190,  0.0862],\n",
       "         [-0.1082, -0.2822,  0.7219,  ...,  0.2569,  0.5190,  0.0862],\n",
       "         [-0.1082, -0.2822,  0.7219,  ...,  0.2569,  0.5190,  0.0862]],\n",
       "\n",
       "        [[-0.1024,  0.0510, -0.0954,  ...,  0.1047,  0.0839, -0.0645],\n",
       "         [-0.3876, -0.4432,  0.3016,  ..., -0.0157,  0.2354, -0.1515],\n",
       "         [ 0.0532, -0.1022, -0.2690,  ...,  0.3892, -0.2509,  0.0830],\n",
       "         ...,\n",
       "         [-0.1693, -0.6132,  0.8854,  ...,  0.4986,  0.1319, -0.0186],\n",
       "         [-0.1693, -0.6132,  0.8854,  ...,  0.4986,  0.1319, -0.0186],\n",
       "         [-0.1693, -0.6132,  0.8854,  ...,  0.4986,  0.1319, -0.0186]]]), tensor([[[-1.2270e-02,  2.3083e-02, -1.1235e-01,  ...,  7.7987e-02,\n",
       "           8.3488e-02,  1.0750e-02],\n",
       "         [-2.1361e-01, -8.2803e-01, -1.0561e-01,  ..., -7.8625e-01,\n",
       "           2.0299e-01,  3.8939e-01],\n",
       "         [ 4.1858e-01,  3.6415e-01, -6.1338e-02,  ..., -6.2393e-03,\n",
       "           1.6858e-01, -1.5088e-01],\n",
       "         ...,\n",
       "         [ 1.1188e-01, -4.1883e-01,  8.8558e-01,  ...,  6.0631e-01,\n",
       "           8.2566e-02,  1.8380e-01],\n",
       "         [ 1.1188e-01, -4.1883e-01,  8.8558e-01,  ...,  6.0631e-01,\n",
       "           8.2566e-02,  1.8380e-01],\n",
       "         [ 1.1188e-01, -4.1883e-01,  8.8558e-01,  ...,  6.0631e-01,\n",
       "           8.2566e-02,  1.8380e-01]],\n",
       "\n",
       "        [[-1.7983e-02,  2.0214e-02, -1.2085e-01,  ...,  9.6687e-02,\n",
       "           7.6711e-02,  2.1765e-02],\n",
       "         [ 2.4459e-01, -1.3810e+00, -2.1233e-01,  ..., -9.1796e-02,\n",
       "           3.8299e-01, -4.6843e-02],\n",
       "         [ 7.7529e-01, -1.2344e-01,  2.0635e-02,  ...,  4.2701e-02,\n",
       "           4.1519e-01, -3.3924e-01],\n",
       "         ...,\n",
       "         [ 1.4032e-01, -2.8143e-01,  6.4537e-01,  ...,  8.0493e-01,\n",
       "           1.3887e-01, -9.2384e-02],\n",
       "         [ 1.4032e-01, -2.8143e-01,  6.4537e-01,  ...,  8.0493e-01,\n",
       "           1.3887e-01, -9.2384e-02],\n",
       "         [ 1.4032e-01, -2.8143e-01,  6.4537e-01,  ...,  8.0493e-01,\n",
       "           1.3887e-01, -9.2384e-02]],\n",
       "\n",
       "        [[-1.6137e-02,  1.8792e-02, -9.4585e-02,  ...,  6.5082e-02,\n",
       "           7.1401e-02,  1.6180e-02],\n",
       "         [-2.2146e-01, -7.5115e-01, -2.5336e-01,  ..., -6.6558e-01,\n",
       "           2.7699e-01, -4.9934e-01],\n",
       "         [-3.9179e-03,  1.5056e-01, -8.1822e-01,  ..., -5.4965e-01,\n",
       "           2.6310e-01, -3.1195e-01],\n",
       "         ...,\n",
       "         [-1.8010e-02, -9.4791e-02,  7.9750e-01,  ...,  1.1616e-01,\n",
       "           3.9482e-01, -4.3921e-02],\n",
       "         [-1.8010e-02, -9.4791e-02,  7.9750e-01,  ...,  1.1616e-01,\n",
       "           3.9482e-01, -4.3921e-02],\n",
       "         [-1.8010e-02, -9.4791e-02,  7.9750e-01,  ...,  1.1616e-01,\n",
       "           3.9482e-01, -4.3921e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-8.9715e-03,  2.1554e-02, -8.2803e-02,  ...,  7.2232e-02,\n",
       "           8.2872e-02,  1.5445e-02],\n",
       "         [-2.4392e-02, -1.1362e+00, -3.4089e-01,  ...,  2.6719e-03,\n",
       "           2.3576e-01,  8.9315e-01],\n",
       "         [-5.1894e-02,  3.7617e-01,  3.1521e-01,  ..., -1.2635e-01,\n",
       "          -8.4075e-02,  2.0085e-01],\n",
       "         ...,\n",
       "         [ 7.2122e-01, -3.3520e-01,  3.2726e-01,  ...,  4.0874e-01,\n",
       "           5.3085e-01, -2.0722e-01],\n",
       "         [ 7.2122e-01, -3.3520e-01,  3.2726e-01,  ...,  4.0874e-01,\n",
       "           5.3085e-01, -2.0722e-01],\n",
       "         [ 7.2122e-01, -3.3520e-01,  3.2726e-01,  ...,  4.0874e-01,\n",
       "           5.3085e-01, -2.0722e-01]],\n",
       "\n",
       "        [[ 6.2726e-04,  1.4902e-02, -1.0203e-01,  ...,  8.4757e-02,\n",
       "           7.1334e-02,  4.6084e-03],\n",
       "         [-1.9662e-01, -8.3538e-01, -3.5189e-02,  ..., -6.7291e-01,\n",
       "           1.0709e-01,  1.3302e-01],\n",
       "         [ 3.5267e-01,  2.0085e-01, -1.6041e-01,  ..., -2.9381e-01,\n",
       "           6.4232e-01,  5.4143e-02],\n",
       "         ...,\n",
       "         [-1.3618e-01, -5.6924e-01,  5.5591e-01,  ...,  4.2554e-01,\n",
       "          -1.4541e-01, -1.3415e-02],\n",
       "         [-1.3618e-01, -5.6924e-01,  5.5591e-01,  ...,  4.2554e-01,\n",
       "          -1.4541e-01, -1.3415e-02],\n",
       "         [-1.3618e-01, -5.6924e-01,  5.5591e-01,  ...,  4.2554e-01,\n",
       "          -1.4541e-01, -1.3415e-02]],\n",
       "\n",
       "        [[-4.7568e-02,  4.1461e-02, -9.3473e-02,  ...,  4.7841e-02,\n",
       "           7.7643e-02, -1.1129e-02],\n",
       "         [-5.6939e-01, -7.7774e-01, -6.6361e-02,  ...,  2.9712e-01,\n",
       "           8.7031e-01, -1.1672e-01],\n",
       "         [ 1.2923e-01,  2.9154e-01,  3.1555e-01,  ...,  3.1192e-01,\n",
       "          -4.6925e-02,  4.7387e-02],\n",
       "         ...,\n",
       "         [-2.6088e-01, -8.4597e-01,  6.8622e-01,  ...,  4.8297e-01,\n",
       "          -1.4584e-01, -6.4618e-02],\n",
       "         [-2.6088e-01, -8.4597e-01,  6.8622e-01,  ...,  4.8297e-01,\n",
       "          -1.4584e-01, -6.4618e-02],\n",
       "         [-2.6088e-01, -8.4597e-01,  6.8622e-01,  ...,  4.8297e-01,\n",
       "          -1.4584e-01, -6.4618e-02]]]), tensor([[[-0.0245, -0.0141, -0.0316,  ...,  0.0718, -0.0810, -0.0020],\n",
       "         [-0.1081, -0.6114, -0.1655,  ..., -0.5638,  0.2558,  0.3755],\n",
       "         [ 0.2772,  0.0889,  0.0990,  ...,  0.0559, -0.0867, -0.1711],\n",
       "         ...,\n",
       "         [ 0.2657, -0.3743,  0.5441,  ...,  0.5190,  0.2103,  0.1760],\n",
       "         [ 0.2657, -0.3743,  0.5441,  ...,  0.5190,  0.2103,  0.1760],\n",
       "         [ 0.2657, -0.3743,  0.5441,  ...,  0.5190,  0.2103,  0.1760]],\n",
       "\n",
       "        [[-0.0168, -0.0114, -0.0247,  ...,  0.0789, -0.0658,  0.0123],\n",
       "         [ 0.3054, -1.2795, -0.4629,  ...,  0.0582,  0.7644, -0.2613],\n",
       "         [ 0.4076, -0.1376,  0.0276,  ..., -0.0117,  0.5602, -0.3662],\n",
       "         ...,\n",
       "         [ 0.2477, -0.2687,  0.3326,  ...,  0.6717,  0.2096,  0.0328],\n",
       "         [ 0.2477, -0.2687,  0.3326,  ...,  0.6717,  0.2096,  0.0328],\n",
       "         [ 0.2477, -0.2687,  0.3326,  ...,  0.6717,  0.2096,  0.0328]],\n",
       "\n",
       "        [[-0.0188, -0.0187, -0.0283,  ...,  0.0686, -0.0942,  0.0193],\n",
       "         [ 0.1073, -0.9522, -0.4278,  ..., -0.3257,  0.5690, -0.1785],\n",
       "         [ 0.0292,  0.1051, -0.3978,  ..., -0.4502,  0.3059, -0.0467],\n",
       "         ...,\n",
       "         [ 0.1217, -0.1657,  0.2929,  ...,  0.1219,  0.0668, -0.1809],\n",
       "         [ 0.1217, -0.1657,  0.2929,  ...,  0.1219,  0.0668, -0.1809],\n",
       "         [ 0.1217, -0.1657,  0.2929,  ...,  0.1219,  0.0668, -0.1809]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0114, -0.0211, -0.0358,  ...,  0.0814, -0.0862,  0.0092],\n",
       "         [ 0.2791, -1.2642, -0.1704,  ...,  0.0034,  0.0914,  0.5565],\n",
       "         [-0.1617,  0.2186,  0.1520,  ...,  0.0039, -0.1698,  0.3629],\n",
       "         ...,\n",
       "         [ 0.4626, -0.4278,  0.3535,  ...,  0.5369,  0.3882, -0.2621],\n",
       "         [ 0.4626, -0.4278,  0.3535,  ...,  0.5369,  0.3882, -0.2621],\n",
       "         [ 0.4626, -0.4278,  0.3535,  ...,  0.5369,  0.3882, -0.2621]],\n",
       "\n",
       "        [[-0.0106, -0.0136, -0.0250,  ...,  0.0843, -0.0961,  0.0122],\n",
       "         [-0.1943, -0.6615, -0.3050,  ..., -0.4821,  0.1981,  0.1381],\n",
       "         [ 0.1663, -0.1707, -0.0455,  ..., -0.2976,  0.2541,  0.0023],\n",
       "         ...,\n",
       "         [ 0.1094, -0.4528,  0.3602,  ...,  0.3795,  0.0509,  0.0931],\n",
       "         [ 0.1094, -0.4528,  0.3602,  ...,  0.3795,  0.0509,  0.0931],\n",
       "         [ 0.1094, -0.4528,  0.3602,  ...,  0.3795,  0.0509,  0.0931]],\n",
       "\n",
       "        [[-0.0163, -0.0093, -0.0300,  ...,  0.0547, -0.1018,  0.0153],\n",
       "         [-0.8048, -0.3811, -0.2238,  ...,  0.4256,  0.3373, -0.1341],\n",
       "         [ 0.0909,  0.0781,  0.3039,  ...,  0.2957, -0.1136,  0.0533],\n",
       "         ...,\n",
       "         [ 0.0979, -0.6995,  0.4018,  ...,  0.3663,  0.0724,  0.1081],\n",
       "         [ 0.0979, -0.6995,  0.4018,  ...,  0.3663,  0.0724,  0.1081],\n",
       "         [ 0.0979, -0.6995,  0.4018,  ...,  0.3663,  0.0724,  0.1081]]]), tensor([[[-0.0110,  0.0662, -0.0402,  ..., -0.0765, -0.0393,  0.0051],\n",
       "         [ 0.0246, -0.1859, -0.0735,  ..., -0.3455,  0.1115,  0.1199],\n",
       "         [ 0.0491,  0.0213,  0.0921,  ..., -0.0817,  0.0128, -0.0482],\n",
       "         ...,\n",
       "         [ 0.0790,  0.0204,  0.0805,  ...,  0.0735,  0.0150,  0.0238],\n",
       "         [ 0.0790,  0.0204,  0.0805,  ...,  0.0735,  0.0150,  0.0238],\n",
       "         [ 0.0790,  0.0204,  0.0805,  ...,  0.0735,  0.0150,  0.0238]],\n",
       "\n",
       "        [[-0.0115,  0.0960, -0.0436,  ..., -0.0642, -0.0510, -0.0248],\n",
       "         [ 0.1513, -0.4553, -0.0328,  ..., -0.1751,  0.3167, -0.2626],\n",
       "         [ 0.2684, -0.0636,  0.1094,  ..., -0.0759,  0.2083, -0.1840],\n",
       "         ...,\n",
       "         [ 0.1074,  0.0680,  0.0232,  ...,  0.1364, -0.0319, -0.0557],\n",
       "         [ 0.1074,  0.0680,  0.0232,  ...,  0.1364, -0.0319, -0.0557],\n",
       "         [ 0.1074,  0.0680,  0.0232,  ...,  0.1364, -0.0319, -0.0557]],\n",
       "\n",
       "        [[-0.0231,  0.0543, -0.0471,  ..., -0.1168, -0.0329, -0.0221],\n",
       "         [-0.0257, -0.3383, -0.0398,  ..., -0.2831,  0.1596, -0.1052],\n",
       "         [ 0.1653,  0.0165, -0.0106,  ..., -0.2187,  0.2102, -0.0407],\n",
       "         ...,\n",
       "         [ 0.0328, -0.0216,  0.0735,  ..., -0.1318, -0.0123,  0.0299],\n",
       "         [ 0.0328, -0.0216,  0.0735,  ..., -0.1318, -0.0123,  0.0299],\n",
       "         [ 0.0328, -0.0216,  0.0735,  ..., -0.1318, -0.0123,  0.0299]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0392,  0.0620, -0.0396,  ..., -0.0945, -0.0609, -0.0203],\n",
       "         [ 0.0842, -0.4585, -0.2374,  ..., -0.2029,  0.0597,  0.1723],\n",
       "         [-0.0554,  0.1015,  0.0998,  ..., -0.4048, -0.0006,  0.2756],\n",
       "         ...,\n",
       "         [ 0.1201, -0.0252,  0.0802,  ...,  0.0705,  0.0877, -0.0928],\n",
       "         [ 0.1201, -0.0252,  0.0802,  ...,  0.0705,  0.0877, -0.0928],\n",
       "         [ 0.1201, -0.0252,  0.0802,  ...,  0.0705,  0.0877, -0.0928]],\n",
       "\n",
       "        [[-0.0244,  0.0636, -0.0208,  ..., -0.1070, -0.0254, -0.0335],\n",
       "         [-0.0851, -0.2687, -0.1086,  ..., -0.4214,  0.1786,  0.0464],\n",
       "         [ 0.0725, -0.1549,  0.1507,  ..., -0.1649,  0.1020,  0.0540],\n",
       "         ...,\n",
       "         [ 0.0575, -0.0138,  0.0420,  ..., -0.0333, -0.0209, -0.0271],\n",
       "         [ 0.0575, -0.0138,  0.0420,  ..., -0.0333, -0.0209, -0.0271],\n",
       "         [ 0.0575, -0.0138,  0.0420,  ..., -0.0333, -0.0209, -0.0271]],\n",
       "\n",
       "        [[-0.0564,  0.0435, -0.0183,  ..., -0.1650, -0.0948, -0.0094],\n",
       "         [ 0.0018,  0.1074, -0.0314,  ..., -0.0649, -0.0318, -0.1093],\n",
       "         [-0.0212, -0.0546, -0.1059,  ...,  0.0622, -0.2630,  0.0517],\n",
       "         ...,\n",
       "         [ 0.0290, -0.1055,  0.0856,  ..., -0.0795, -0.1458,  0.0297],\n",
       "         [ 0.0290, -0.1055,  0.0856,  ..., -0.0795, -0.1458,  0.0297],\n",
       "         [ 0.0290, -0.1055,  0.0856,  ..., -0.0795, -0.1458,  0.0297]]])), past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d481589-d710-4c61-a351-c5f0b74675af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
