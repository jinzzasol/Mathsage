{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a5bb58",
   "metadata": {},
   "source": [
    "# MathQA Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806a42d",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Maybe increase problem embedding by a factor of 3?\n",
    "- Change masked num embeddings function so it usese predicted constants from constants.pickle\n",
    "- Consider doing the same for the operators\n",
    "- Finish forward propogation (test a pass)\n",
    "- Write custom loss function\n",
    "- test 1 full pass of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10e4a1",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fffb4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os\n",
    "import anytree\n",
    "from anytree import RenderTree\n",
    "from anytree.importer import DictImporter\n",
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9991e",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d428a038",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Enum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m WORKING_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEMP/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m OBJ_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickle/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOp\u001b[39;00m(\u001b[43mEnum\u001b[49m):\n\u001b[1;32m     17\u001b[0m     ADD \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m     SUB \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Enum' is not defined"
     ]
    }
   ],
   "source": [
    "K = 6\n",
    "MAX_LAYERS = 8\n",
    "MAX_TOKENS = 392\n",
    "EMBEDDING_SIZE = 768\n",
    "\n",
    "DATA_PATH = './dataset/'\n",
    "SET_NAMES = ['train', 'validation', 'test']\n",
    "ENCODER_MODEL = 'distilroberta-base' # A more optimized version of roberta obtaining 95% of its performance\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_MASK = '<num>'\n",
    "WORKING_DIR = 'TEMP/'\n",
    "\n",
    "OBJ_DIR = 'pickle/'\n",
    "\n",
    "\n",
    "class Op(Enum):\n",
    "    ADD = '+'\n",
    "    SUB = '-'\n",
    "    MULT = '*'\n",
    "    DIV = '/'\n",
    "    POW = '^'\n",
    "    \n",
    "class Const(Enum):\n",
    "    CONST_NEG_1 = 'const_neg_1' # I added this\n",
    "    CONST_0_25 = 'const_0_25'\n",
    "    CONST_0_2778 = 'const_0_2778'\n",
    "    CONST_0_33 = 'const_0_33'\n",
    "    CONST_0_3937 = 'const_0_3937'\n",
    "    CONST_1 = 'const_1'\n",
    "    CONST_1_6 = 'const_1_6'\n",
    "    CONST_2 = 'const_2'\n",
    "    CONST_3 = 'const_3'\n",
    "    CONST_PI = 'const_pi'\n",
    "    CONST_3_6 = 'const_3_6'\n",
    "    CONST_4 = 'const_4'\n",
    "    CONST_5 = 'const_5'\n",
    "    CONST_6 = 'const_6'\n",
    "    CONST_10 = 'const_10'\n",
    "    CONST_12 = 'const_12'\n",
    "    CONST_26 = 'const_26'\n",
    "    CONST_52 = 'const_52'\n",
    "    CONST_60 = 'const_60'\n",
    "    CONST_100 = 'const_100'\n",
    "    CONST_180 = 'const_180'\n",
    "    CONST_360 = 'const_360'\n",
    "    CONST_1000 = 'const_1000'\n",
    "    CONST_3600 = 'const_3600'\n",
    "\n",
    "values = [-1, 0.25, 0.2778, 0.33, 0.3937, 1, 1.6, 2, 3, math.pi, 3.6, 4, 5, 6, 10, 12, 26, 52, 60, 100, 180, 360, 1000, 3600]\n",
    "const2val = {k:v for k,v in zip(Const._value2member_map_.keys(), values)}    \n",
    "\n",
    "op2id = {k:v for k,v in zip(Op._value2member_map_.keys(), range(len(Op._value2member_map_)))}\n",
    "op2id['None'] = 5\n",
    "const2id = {k:v for k,v in zip(Const._value2member_map_.keys(), range(len(Const._value2member_map_)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda851b7",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5bd5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {name:pd.read_csv(f'{DATA_PATH}{name}.csv') for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad07be",
   "metadata": {},
   "source": [
    "## Embedding Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a415f6",
   "metadata": {},
   "source": [
    "Converting number embeddings to masked tensors (to ensure they are all homogeneous) and adding in constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7736e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_masked_embeddings(name):\n",
    "#     num_embed = embeddings[name]['num']\n",
    "#     const_embed = embeddings[name]['num']\n",
    "#     mapping, nums = embeddings[name]['num_mapping']\n",
    "#     const = [[num for num in eval(x) if num in const2val] for x in data[name]['nums']]\n",
    "#     max_nums = (np.bincount(mapping)+np.array(list(map(len, const)))).max()\n",
    "\n",
    "#     result = () \n",
    "#     for idx in range(len(const)):\n",
    "#         # Getting number embeddings\n",
    "#         nums = num_embed[mapping==idx]\n",
    "\n",
    "#         # Adding constant embeddings\n",
    "#         c = tuple([const_embed[const2id[x]] for x in const[idx]])\n",
    "#         if len(c) > 0:\n",
    "#             c = torch.stack(c)\n",
    "#             nums = torch.cat((nums, c), dim=0)\n",
    "        \n",
    "#           # non masked code\n",
    "# #         if result is None:\n",
    "# #             result = nums\n",
    "# #         else:\n",
    "# #             result = torch.cat((result, nums), dim=0)\n",
    "        \n",
    "#         # Padding and creating mask\n",
    "#         dim1, dim2 = nums.shape\n",
    "#         mask = torch.full((dim1,dim2), True)\n",
    "#         nums = F.pad(nums, (0,0,0,max_nums-dim1), 'constant', 0)\n",
    "#         mask = F.pad(mask, (0,0,0,max_nums-dim1), 'constant', False)\n",
    "        \n",
    "#         # Creating masked tensor object\n",
    "#         mt = torch.masked.masked_tensor(nums, mask)[None,:,:]\n",
    "#         result = result + (mt,)\n",
    "        \n",
    "#     return torch.cat(result, dim=0)\n",
    "# masked_num_embed = {name:create_masked_embeddings(name) for name in SET_NAMES}\n",
    "# masked_num_embed['train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081437d6",
   "metadata": {},
   "source": [
    "## Constructing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c533b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerDecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, num_tokens, max_layers, K):\n",
    "        super(MultiLayerDecoder, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.max_layers = max_layers\n",
    "        self.K = K\n",
    "        self.decoder_layer = MathQADecoder(embedding_size, num_tokens, K)\n",
    "    \n",
    "    # Repeatedly get new sets of equations until a maximum depth is reached or there are no more valid equations (all ops are None)\n",
    "    def forward(self, x, embeddings):\n",
    "        \n",
    "        x = x.to(self.device)\n",
    "        nums = embeddings['nums'].to(self.device)\n",
    "        num_idx = embeddings['num_idx'].to(self.device)\n",
    "        ops = embeddings['ops'].to(self.device)\n",
    "        problems = embeddings['problem'].to(self.device)\n",
    "\n",
    "        # while less than max layers and the input has more elements\n",
    "        idx = 0\n",
    "        temp = [] # REMOVE THIS\n",
    "        while idx < self.max_layers and x.numel():\n",
    "            prev_idx = num_idx\n",
    "            \n",
    "            # --------------------------------------------------\n",
    "            # Step 1 - Encoding to embedding size if not already\n",
    "            # --------------------------------------------------\n",
    "            if x.shape[-1] == self.embedding_size*4:\n",
    "                x = self.decoder_layer.exp_encoder(x) #3 [batch_size, K, 3072] -> [batch_size, K, 768] \n",
    "\n",
    "            # -----------------------\n",
    "            # Step 2 - The main model\n",
    "            # -----------------------\n",
    "            prev_sizes = num_idx.bincount()\n",
    "            x, op, num1, num2, nums, num_idx, problems = self.decoder_layer(x, nums, num_idx, ops, problems)\n",
    "            new_sizes = num_idx.bincount()\n",
    "            \n",
    "            # ---------------------------------------------------------------------------------------------------------------------------------\n",
    "            # Step 3 - Removing examples with no valid expressions (If the num nums for a problem is unchanged, it had no valid expressions)\n",
    "            # ---------------------------------------------------------------------------------------------------------------------------------\n",
    "            changed = prev_sizes!=new_sizes # [batch_size] (not_finished True, finished False)\n",
    "            num_idx = num_idx[changed[num_idx]] # [num_nums_not_finished]\n",
    "            nums = nums[num_idx]  # [num_nums_not_finished, 768] \n",
    "            problems = problems[changed] # [not_finished, num_tokens, 768]\n",
    "            x = x[changed] # [not_finished, K, 768]\n",
    "\n",
    "            # print(f'x: {x.shape}')\n",
    "            # print(f'op: {op.shape}')\n",
    "            # print(f'num1: {num1.shape}')\n",
    "            # print(f'num2: {num2.shape}')\n",
    "            # print(f'nums: {nums.shape}')\n",
    "            # print(f'num_idx: {num_idx.shape}')\n",
    "            # print(f'problems: {problems.shape}')\n",
    "            # print()\n",
    "\n",
    "            idx += 1\n",
    "            temp.append((op,num1,num2,prev_idx))\n",
    "\n",
    "        return temp\n",
    "\n",
    "        #return op, num1, num2, prev_idx\n",
    "        \n",
    "\n",
    "class MathQADecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, num_tokens, K): \n",
    "        super(MathQADecoder, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.K = K\n",
    "        \n",
    "        # decreasing dimensionality to match embedding size: <op, num, num, num*num> = 3072 -> 768\n",
    "        self.exp_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_size*4, embedding_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # converting back into new <op, num, num> for loss calculation\n",
    "        self.exp_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_size, embedding_size*3),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Mixing in expression information to the problem encoding\n",
    "        self.prob_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_tokens+K, num_tokens),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # standard transformer decoder\n",
    "        # we choose K heads for K generated expressions \n",
    "        # (with the hope that each head will get different information for each K expression)\n",
    "        self.transformer_decoder = torch.nn.TransformerDecoderLayer(embedding_size, nhead=K, batch_first=True)\n",
    "        \n",
    "        # softmax for the operations\n",
    "        self.op_softmax = torch.nn.Softmax(dim=2)\n",
    "        \n",
    "        # softmax for the numbers\n",
    "        self.num_softmax = torch.nn.Softmax(dim=0)\n",
    "        \n",
    "    def __apply_to_nums(self, f, nums, num_idx):\n",
    "        return torch.cat(tuple([f(nums[num_idx==x]) for x in range(nums.shape[0])]), dim=0)\n",
    "      \n",
    "    # x: [batch_size, K, 768]\n",
    "    # nums: [num_nums, 768]\n",
    "    # num_idx: [num_nums,]\n",
    "    # ops: [num_ops, 768]\n",
    "    # problems: [batch_size, num_tokens, 768]\n",
    "    def forward(self, x, nums, num_idx, ops, problems):    \n",
    "        batch_size = x.shape[0]\n",
    "        num_ops = ops.shape[0]\n",
    "        num_nums = nums.shape[0]\n",
    "\n",
    "        # ----------------------------\n",
    "        # Step 1 - transformer decoder\n",
    "        # ----------------------------\n",
    "        x = self.transformer_decoder(x, problems) # [batch_size, K, 768] -> [batch_size, K, 768] (problems is [batch_size, num_tokens, 768])\n",
    "\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # Step 2 - decoding the output into three embeddings of size 768 (op, num1, num2)\n",
    "        # -------------------------------------------------------------------------------\n",
    "        x = self.exp_decoder(x) # [batch_size, K, 768] -> [batch_size, K, 2304]\n",
    "        operation, x1, x2 = torch.split(x, self.embedding_size, dim=2) # [batch_size, K, 2304] -> [batch_size, K, 768] for each\n",
    "\n",
    "        # -----------------------------------------------------------------------------------------------\n",
    "        # Step 3 and 4 - Finding the softmax for the similarity between the predicted and true embeddings\n",
    "        # -----------------------------------------------------------------------------------------------\n",
    "        # making sure params have correct dimension\n",
    "        nums_expanded = nums[:,None,:].expand(-1,self.K,-1) # [number_of_nums, 768] -> [number_of_nums, K, 768]\n",
    "        ops = ops[None,None,:,:].repeat(batch_size,self.K,1,1) # [num_ops, 768] -> [batch_size, K, num_ops, 768]\n",
    "\n",
    "        # dot product - calculating the similarity between each op/num prediction and its true embedding\n",
    "        num1 = (x1[num_idx]*nums_expanded).sum(dim=2) # [number_of_nums, K]\n",
    "        num2 = (x2[num_idx]*nums_expanded).sum(dim=2) # [number_of_nums, K]        \n",
    "        op = operation[:,:,None,:].expand(-1,-1,num_ops,-1) # [batch_size, K, 768] -> # [batch_size, K, num_ops, 768]\n",
    "        op = (op*ops).sum(dim=3) # [batch_size, K, num_ops, 768] -> [batch_size, K, num_ops]\n",
    "\n",
    "        # softmax\n",
    "        op = self.op_softmax(op) # [batch_size, K, num_ops] (ie op[1,2] would be the operator prediction probabilities for problem2, query3)\n",
    "        num1 = self.__apply_to_nums(self.num_softmax, num1, num_idx) # [num_nums, K]\n",
    "        num2 = self.__apply_to_nums(self.num_softmax, num2, num_idx) # [num_nums, K]\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 5 - creating embedding for the found expression  \n",
    "        # ----------------------------------------------------\n",
    "        x = self.exp_encoder(torch.cat((operation,x1,x2,x1*x2), dim=2)) # [batch_size, K, 3072] -> [batch_size, K, 768]  \n",
    "\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "        # Step 6 - finding valid, adding found expression to problem embeddings and number embeddings\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "        # Getting predicted ops\n",
    "        mask = torch.argmax(op, dim=2)!=op2id['None'] # [batch_size, K]\n",
    "\n",
    "        # Getting valid embeddings along with a problem index\n",
    "        valid = x[mask] # [num_valid, 768]\n",
    "        problem_idx = torch.arange(mask.shape[0]).to(self.device).repeat_interleave(num_ops)[mask.flatten()] # [num_valid, 768]\n",
    "\n",
    "        # Appending to num_embeddings\n",
    "        nums = torch.cat((nums, valid), dim=0) # [num_nums+num_valid, 768]\n",
    "        num_idx = torch.cat((num_idx, problem_idx), dim=0) # [num_nums+num_valid,]\n",
    "\n",
    "        # Updating problem embeddings\n",
    "        temp = torch.clone(x)\n",
    "        temp[~mask] = 0 # [batch_size, K, 768]\n",
    "        problems = torch.cat((problems,temp),dim=1) # [batch_size, num_tokens+K, 768]\n",
    "        problems = self.prob_encoder(problems.permute(0,2,1)) # [batch_size, 768, num_tokens]\n",
    "        problems = problems.permute(0,2,1) # [batch_size, num_tokens, 768]\n",
    "\n",
    "        # -----------------------\n",
    "        # Returning final results\n",
    "        # -----------------------\n",
    "        return x, op, num1, num2, nums, num_idx, problems\n",
    "    \n",
    "    \"\"\"  \n",
    "        # Needed for loss calculation\n",
    "        # pretty sure this might be wrong now\n",
    "    \n",
    "        # getting correct op/nums idx\n",
    "        op = torch.argmax(op, dim=2) # [batch_size, K, num_ops] -> [batch_size, K]\n",
    "        num1 = torch.argmax(op, dim=2) # [batch_size, K, num_nums] -> [batch_size, K]\n",
    "        num2 = torch.argmax(op, dim=2) # [batch_size, K, num_nums] -> [batch_size, K]\n",
    "        \n",
    "        # getting correct op/nums embeddings\n",
    "        temp = torch.arange(temp.size(0)).repeat_interleave(K).reshape(batch_size, K) # [batch_size, K]\n",
    "        op = ops[temp, op] # [batch_size, K] -> [batch_size, K, 768]\n",
    "        num1 = nums[temp, op] # [batch_size, K] -> [batch_size, K, 768]\n",
    "        num2 = nums[temp, op] # [batch_size, K] -> [batch_size, K, 768]\n",
    "    \n",
    "        return x\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8221de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: [batch_size, K, 768]\n",
    "# nums: [num_nums, 768]\n",
    "# num_idx: [num_nums,]\n",
    "# ops: [num_ops, 768]\n",
    "# problems: [batch_size, num_tokens, 768]\n",
    "try:\n",
    "    with open(f'{OBJ_DIR}embeddings/train/batch0.pickle', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    x = torch.rand(8,6,768)\n",
    "    model = MultiLayerDecoder(768, 392, 8, 6)\n",
    "    model.to('cuda:0')\n",
    "    result = model(x, embeddings)\n",
    "finally:\n",
    "    del x\n",
    "    #del embeddings\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ffafb97-a5d9-4483-a56c-0801cfc387fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 4, 1, 4, 0, 3, 1, 1, 2, 4, 5, 3, 5, 5, 4, 0, 1, 5, 4, 4, 1, 3, 1,\n",
       "        2], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op,num1,num2,num_idx=result[0]\n",
    "op.argmax(dim=2)\n",
    "torch.cat([num1[num_idx==x].argmax(dim=-1) for x in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f066e22e-81ff-407b-812d-6d7bd3fd86bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3330,  1.9565, -1.2056,  ..., -0.8565,  0.8773,  1.1536],\n",
       "        [ 0.6438,  1.8634, -0.7155,  ...,  0.6324,  1.4042,  1.3915],\n",
       "        [ 0.2300,  1.3455, -0.7449,  ...,  2.3113,  1.2338,  2.6616]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['nums'][embeddings['num_idx']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a43ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OBJ_DIR}embeddings/train/batch0.pickle', 'rb') as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8d020c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 392])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e31f75db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 0, 3, 3, 4, 4],\n",
       "        [4, 3, 3, 3, 3, 0],\n",
       "        [0, 4, 0, 4, 4, 4],\n",
       "        [3, 0, 4, 3, 4, 4],\n",
       "        [0, 4, 3, 3, 3, 4],\n",
       "        [3, 4, 0, 3, 0, 0],\n",
       "        [3, 3, 0, 4, 0, 4],\n",
       "        [3, 4, 3, 0, 4, 3]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e3fd4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 2, 5, 5, 5, 5],\n",
       "        [3, 5, 5, 5, 5, 5],\n",
       "        [2, 5, 5, 5, 5, 5],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_ids = np.zeros((MAX_LAYERS, len(data['train']),K), dtype=int)\n",
    "i = 0\n",
    "for j, step in enumerate(data['train']['incremental'][i].split(' ; ')):\n",
    "    for k, x in enumerate(eval(step)):\n",
    "        if x is not None:\n",
    "            x1, op, x2 = x.split()\n",
    "            op_ids[i,j,k] = op2id[op]\n",
    "        else:\n",
    "            op_ids[i,j,k] = op2id['None']\n",
    "op_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "089636b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for x in data['train']['incremental'].dropna():\n",
    "    temp = len(x.split(' ; '))\n",
    "    if temp > 8:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a40662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
