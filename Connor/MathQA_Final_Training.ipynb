{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc963111",
   "metadata": {},
   "source": [
    "# MathQA Final Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7d716",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c82851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import pickle\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import unittest\n",
    "import torch\n",
    "import evaluate\n",
    "import anytree\n",
    "from anytree.importer import DictImporter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8766eef1",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e9b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "MAX_LAYERS = 8\n",
    "MAX_TOKENS = 392\n",
    "EMBEDDING_SIZE = 768\n",
    "\n",
    "DATA_PATH = './dataset/'\n",
    "SET_NAMES = ['train', 'validation', 'test']\n",
    "MODEL = 'google/flan-t5-large'\n",
    "MODEL_PATH = f'models/{MODEL.split(\"/\")[-1]}-MathQA'\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_MASK = '<num>'\n",
    "WORKING_DIR = 'TEMP/'\n",
    "\n",
    "OBJ_DIR = 'pickle/'\n",
    "\n",
    "\n",
    "class Op(Enum):\n",
    "    ADD = '+'\n",
    "    SUB = '-'\n",
    "    MULT = '*'\n",
    "    DIV = '/'\n",
    "    POW = '^'\n",
    "    \n",
    "class Const(Enum):\n",
    "    CONST_NEG_1 = 'const_neg_1' # I added this\n",
    "    CONST_0_25 = 'const_0_25'\n",
    "    CONST_0_2778 = 'const_0_2778'\n",
    "    CONST_0_33 = 'const_0_33'\n",
    "    CONST_0_3937 = 'const_0_3937'\n",
    "    CONST_1 = 'const_1'\n",
    "    CONST_1_6 = 'const_1_6'\n",
    "    CONST_2 = 'const_2'\n",
    "    CONST_3 = 'const_3'\n",
    "    CONST_PI = 'const_pi'\n",
    "    CONST_3_6 = 'const_3_6'\n",
    "    CONST_4 = 'const_4'\n",
    "    CONST_5 = 'const_5'\n",
    "    CONST_6 = 'const_6'\n",
    "    CONST_10 = 'const_10'\n",
    "    CONST_12 = 'const_12'\n",
    "    CONST_26 = 'const_26'\n",
    "    CONST_52 = 'const_52'\n",
    "    CONST_60 = 'const_60'\n",
    "    CONST_100 = 'const_100'\n",
    "    CONST_180 = 'const_180'\n",
    "    CONST_360 = 'const_360'\n",
    "    CONST_1000 = 'const_1000'\n",
    "    CONST_3600 = 'const_3600'\n",
    "\n",
    "values = [-1, 0.25, 0.2778, 0.33, 0.3937, 1, 1.6, 2, 3, 3.1416, 3.6, 4, 5, 6, 10, 12, 26, 52, 60, 100, 180, 360, 1000, 3600]\n",
    "const2val = {k:v for k,v in zip(Const._value2member_map_.keys(), values)}    \n",
    "\n",
    "op2id = {k:v for k,v in zip(Op._value2member_map_.keys(), range(len(Op._value2member_map_)))}\n",
    "op2id['None'] = 5\n",
    "id2op = np.array(list(op2id.keys()))\n",
    "const2id = {k:v for k,v in zip(Const._value2member_map_.keys(), range(len(Const._value2member_map_)))}\n",
    "id2const = np.array(list(const2id.keys()))\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "class Util():\n",
    "    def load_obj(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            o = pickle.load(f)\n",
    "        return o\n",
    "    \n",
    "    def save_obj(self, path, o):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(o, f)\n",
    "            \n",
    "    def load_data(self):\n",
    "        return {name:pd.read_csv(f'{DATA_PATH}{name}.csv') for name in SET_NAMES}\n",
    "    \n",
    "    def set_seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "util = Util()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186e74d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af101dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(arr):\n",
    "    idx = np.concatenate([[i]*len(x) for i,x in enumerate(arr)])\n",
    "    flattened = np.concatenate(arr) \n",
    "    return idx, flattened\n",
    "\n",
    "def get_nums_and_mask(data):\n",
    "    nums = {name:[] for name in SET_NAMES}\n",
    "    problems = {name:[] for name in SET_NAMES}\n",
    "    num_idx = {name:[] for name in SET_NAMES}\n",
    "    for name in SET_NAMES:\n",
    "        for i,problem in enumerate(data[name].problem):\n",
    "            num = re.compile('([+-]?((\\d+(\\.\\d*)?)|(\\.\\d+)))') # normal num\n",
    "            big = re.compile(r'(-?\\d{1,3}(,\\d{3})+(\\.\\d*)?)') # num with comma\n",
    "\n",
    "            big_results = re.finditer(big, problem)\n",
    "            problem = re.sub(big, NUM_MASK, problem)        \n",
    "            num_results = re.finditer(num, problem)\n",
    "            problem = re.sub(num, NUM_MASK, problem)\n",
    "\n",
    "            # Getting the combined numbers in order of occurence\n",
    "            combined = [x for x in num_results]\n",
    "            combined.extend([x for x in big_results])\n",
    "            combined = sorted(combined, key=lambda x: x.start(0))\n",
    "\n",
    "            combined = [float(x.group(0).replace(',','')) for x in combined]\n",
    "\n",
    "            nums[name].append(np.array(combined))\n",
    "            problems[name].append(problem)\n",
    "        num_idx[name], nums[name] = flatten(np.array(nums[name], dtype=object))\n",
    "        problems[name] = np.array(problems[name])\n",
    "    return {name:{'idx':torch.tensor(num_idx[name]), 'literals':nums[name]} for name in SET_NAMES}, problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49543f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = util.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf84cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = util.load_obj(f'{OBJ_DIR}ops.pickle')\n",
    "const = util.load_obj(f'{OBJ_DIR}constants.pickle')\n",
    "subexp = util.load_obj(f'{OBJ_DIR}subexp.pickle')\n",
    "nums,_ = get_nums_and_mask(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a54b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "def determine_if_int(num):\n",
    "    if float(num).is_integer():\n",
    "        return int(float(num))\n",
    "    else:\n",
    "        return float(num)\n",
    "\n",
    "def str_numpy(arr, t=None):\n",
    "    if t == 'num':\n",
    "        convert = lambda x: str(determine_if_int(x))\n",
    "    elif t == 'eq':\n",
    "        split = lambda x: x.split()\n",
    "        convert_split = lambda x: f'{x[1]} {process_num(x[0])} {process_num(x[2])}'\n",
    "        convert = lambda x: convert_split(split(x))        \n",
    "    else:\n",
    "        convert = lambda x: str(x)\n",
    "        \n",
    "    output = '{'\n",
    "    if len(arr) > 0:\n",
    "        output += f'{convert(arr[0])}'\n",
    "        for x in arr[1:]:\n",
    "            output += f', {convert(x)}'\n",
    "    output += '}'\n",
    "    return output   \n",
    "\n",
    "def process_num(num):\n",
    "    if num in const2val:\n",
    "        return str(determine_if_int(const2val[num]))\n",
    "    else:\n",
    "        return str(determine_if_int(num))\n",
    "    \n",
    "# label preprocessing\n",
    "importer =  DictImporter()\n",
    "def process_item(item):\n",
    "    if item in id2op:\n",
    "        return str(item)\n",
    "    elif item in const2val:\n",
    "        return str(determine_if_int(const2val[item]))\n",
    "    else:\n",
    "        return str(determine_if_int(item))\n",
    "\n",
    "def convert_to_preorder(name):\n",
    "    labels = []\n",
    "    for tree in data[name]['tree']:\n",
    "        root = importer.import_(eval(tree))\n",
    "        output = ''\n",
    "        for node in anytree.PreOrderIter(root):\n",
    "            output += process_item(node.name) + ' '\n",
    "        labels.append(output[0:-1])\n",
    "    return labels\n",
    "     \n",
    "# main function\n",
    "def prompt_engineering(name):\n",
    "    eq, eq_idx = subexp[name]\n",
    "    eq = np.array(eq)\n",
    "    eq_idx = np.array(eq_idx)\n",
    "    idx = nums[name]['idx']\n",
    "    literals = nums[name]['literals']\n",
    "    engineered = []\n",
    "    \n",
    "    for i in range(len(data[name])):\n",
    "        ops = str_numpy(id2op[0:-1][op['pred'][name][i].astype(bool)])\n",
    "        constants = id2const[const['pred'][name][i].astype(bool)]\n",
    "        constants = str_numpy([const2val[x] for x in constants], t='num')\n",
    "        numbers = str_numpy(literals[idx==i], t='num')\n",
    "        equations = str_numpy(eq[eq_idx==i], t='eq')\n",
    "        problem = data[name]['problem'][i]\n",
    "        prompt = f'Find the mathematical formula given: numbers: {numbers}, constants: {constants}, operations: {ops}, and potential subexpressions: {equations} for problem: \"{problem}\"'\n",
    "        engineered.append(prompt)\n",
    "        \n",
    "    return engineered\n",
    "\n",
    "#labels = {name:data[name]['formula_no_const'].str.replace(r'(\\d)\\.0(\\s|\\))', r'\\1\\2', regex=True).str.replace(' ','') for name in SET_NAMES}\n",
    "engineered = {name:Dataset.from_dict({'text':prompt_engineering(name), 'label':convert_to_preorder(name)}) for name in SET_NAMES}\n",
    "engineered = DatasetDict(engineered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c4db0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Find the mathematical formula given: numbers: {3, 10, 36}, constants: {1, 2, 100}, operations: {*, /}, and potential subexpressions: {* 3 10, * 36 100} for problem: \"the banker \\' s gain of a certain sum due 3 years hence at 10 % per annum is rs . 36 . what is the present worth ?\"',\n",
       " 'label': '/ * 100 / * 36 100 * 3 10 * 3 10'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a398607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 18215\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2710\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1798\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c5d53",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad60af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "max_source = 512\n",
    "max_target = 186 # The amount needed to emcompass all equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70dbbd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f50f02c35f44bc8897d86dc569eed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18215 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d1c7b877e54a1bbb19f302ba734280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb24d8ea76b7449cb719afc136982453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenization(batch):\n",
    "    tokenized = tokenizer(text = batch['text'], max_length = max_source, truncation = True)\n",
    "    labels = tokenizer(text_target = batch['label'], max_length = max_source, truncation = True)\n",
    "    tokenized['labels'] = labels['input_ids']   \n",
    "    return tokenized\n",
    "\n",
    "tokenized = engineered.map(tokenization, batched=True, remove_columns=['text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1461ae90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 18215\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2710\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1798\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de8835",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b3796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "grad_acc = 4\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    WORKING_DIR,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=1000,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=5e-5, # learning rates around this worked well for other models trained with this dataset\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    weight_decay=.01, # to maintain consistency with pytorch AdamW optimizer\n",
    "    save_total_limit=3, # ensures dont run out of disk space\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True, # allows the use of rouge and bleu metrics\n",
    "    load_best_model_at_end=True,\n",
    "    optim = 'adamw_bnb_8it' # Have to use a quantized optimizer to make it possible to fit within memory\n",
    "    metric_for_best_model=\"exact_match\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdfd3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load('rouge')\n",
    "bleu_metric = evaluate.load('bleu')\n",
    "def compute_metrics(p):\n",
    "    predictions = p[0]\n",
    "    labels = p[1]\n",
    "    pred_decode = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # the data collator will pad labels with -100 (to signal that they should not be used in loss calculation)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    label_decode = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # rouge (recall)\n",
    "    scores = rouge_metric.compute(predictions=pred_decode, references=label_decode, use_stemmer=False)\n",
    "    \n",
    "    # exact match\n",
    "    exact = (np.array(pred_decode)==np.array(label_decode)).sum()/len(pred_decode)\n",
    "    scores['exact_match'] = exact\n",
    "    \n",
    "    # getting scores\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afa536d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d095eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='7590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 230/7590 06:15 < 3:22:02, 0.61 it/s, Epoch 0.15/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Exact Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.389900</td>\n",
       "      <td>1.139394</td>\n",
       "      <td>0.727257</td>\n",
       "      <td>0.511561</td>\n",
       "      <td>0.649339</td>\n",
       "      <td>0.649453</td>\n",
       "      <td>0.089299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch\\Lib\\site-packages\\transformers\\trainer.py:1815\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1812\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1815\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1816\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch\\Lib\\site-packages\\accelerate\\data_loader.py:393\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 393\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch\\Lib\\site-packages\\accelerate\\utils\\operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m--> 160\u001b[0m         \u001b[43m{\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch\\Lib\\site-packages\\accelerate\\utils\\operations.py:161\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m    160\u001b[0m         {\n\u001b[1;32m--> 161\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    163\u001b[0m         }\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch\\Lib\\site-packages\\accelerate\\utils\\operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94091a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932b3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
