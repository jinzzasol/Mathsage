{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b949f5ef",
   "metadata": {},
   "source": [
    "# MathQA Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8417205d",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92b04d3-cd66-4237-b824-ac18a7debedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os\n",
    "import anytree\n",
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d48fc",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00c0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=6\n",
    "DATA_PATH = './dataset/'\n",
    "SET_NAMES = ['train', 'validation', 'test']\n",
    "ENCODER_MODEL = 'distilroberta-base' # A more optimized version of roberta obtaining 95% of its performance\n",
    "MAX_TOKENS = 392\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_MASK = '<num>'\n",
    "WORKING_DIR = 'TEMP/'\n",
    "FINAL_DIR = 'pickle/'\n",
    "MODEL_DIR = 'models/'\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "class Op(Enum):\n",
    "    ADD = '+'\n",
    "    SUB = '-'\n",
    "    MULT = '*'\n",
    "    DIV = '/'\n",
    "    POW = '^'\n",
    "    \n",
    "class Const(Enum):\n",
    "    CONST_NEG_1 = 'const_neg_1' # I added this\n",
    "    CONST_0_25 = 'const_0_25'\n",
    "    CONST_0_2778 = 'const_0_2778'\n",
    "    CONST_0_33 = 'const_0_33'\n",
    "    CONST_0_3937 = 'const_0_3937'\n",
    "    CONST_1 = 'const_1'\n",
    "    CONST_1_6 = 'const_1_6'\n",
    "    CONST_2 = 'const_2'\n",
    "    CONST_3 = 'const_3'\n",
    "    CONST_PI = 'const_pi'\n",
    "    CONST_3_6 = 'const_3_6'\n",
    "    CONST_4 = 'const_4'\n",
    "    CONST_5 = 'const_5'\n",
    "    CONST_6 = 'const_6'\n",
    "    CONST_10 = 'const_10'\n",
    "    CONST_12 = 'const_12'\n",
    "    CONST_26 = 'const_26'\n",
    "    CONST_52 = 'const_52'\n",
    "    CONST_60 = 'const_60'\n",
    "    CONST_100 = 'const_100'\n",
    "    CONST_180 = 'const_180'\n",
    "    CONST_360 = 'const_360'\n",
    "    CONST_1000 = 'const_1000'\n",
    "    CONST_3600 = 'const_3600'\n",
    "\n",
    "values = [-1, 0.25, 0.2778, 0.33, 0.3937, 1, 1.6, 2, 3, math.pi, 3.6, 4, 5, 6, 10, 12, 26, 52, 60, 100, 180, 360, 1000, 3600]\n",
    "const2val = {k:v for k,v in zip(Const._value2member_map_.keys(), values)}    \n",
    "\n",
    "op2id = {k:v for k,v in zip(Op._value2member_map_.keys(), range(len(Op._value2member_map_)))}\n",
    "const2id = {k:v for k,v in zip(Const._value2member_map_.keys(), range(len(Const._value2member_map_)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5120984",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528e98f",
   "metadata": {},
   "source": [
    "Reading csv into a dictionary of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1239b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {name:pd.read_csv(f'{DATA_PATH}{name}.csv') for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cfd60",
   "metadata": {},
   "source": [
    "Converts operations for each problem into a multi label onehot encoded setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe3a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_ops(data):\n",
    "    labels = []\n",
    "    for op_set in data.ops:\n",
    "        op_set = eval(op_set)\n",
    "        idx = [op2id[op] for op in op_set]\n",
    "        onehot = np.zeros(len(op2id))\n",
    "        onehot[idx] = 1\n",
    "        labels.append(onehot)\n",
    "    return np.array(labels)\n",
    "        \n",
    "#onehot_ops(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93df24d",
   "metadata": {},
   "source": [
    "Sort nums for each each problem in increasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7d18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_num(nums):\n",
    "    get_float = lambda x: float(const2val[x]) if x in const2val else float(x)\n",
    "    return max(map(get_float, nums))\n",
    "\n",
    "def remove_const(data):\n",
    "    nums = []\n",
    "    for num_list in data.nums:\n",
    "        nums.append(set([float(x) for x in eval(num_list) if x not in const2val]))\n",
    "    return nums\n",
    "\n",
    "# Gets the numbers listed in a problem\n",
    "# Once found, numbers are masked using a number mask\n",
    "def get_nums_from_problem(data, convert_to_float=False):\n",
    "    nums = []\n",
    "    problems = []\n",
    "    for problem in data.problem:\n",
    "        num = re.compile('([+-]?((\\d+(\\.\\d*)?)|(\\.\\d+)))')\n",
    "        big = re.compile(r'(-?\\d{1,3}(,\\d{3})+(\\.\\d*)?)')\n",
    "        \n",
    "        big_results = re.finditer(big, problem)\n",
    "        problem = re.sub(big, NUM_MASK, problem)        \n",
    "        num_results = re.finditer(num, problem)\n",
    "        problem = re.sub(num, NUM_MASK, problem)\n",
    "        \n",
    "        # Getting the combined numbers in order of occurence\n",
    "        combined = [x for x in num_results]\n",
    "        combined.extend([x for x in big_results])\n",
    "        combined = sorted(combined, key=lambda x: x.start(0))\n",
    "        \n",
    "        if convert_to_float:\n",
    "            combined = [float(x.group(0).replace(',','')) for x in combined]\n",
    "        else:\n",
    "            combined = [x.group(0) for x in combined]\n",
    "        \n",
    "        nums.append(combined)\n",
    "        problems.append(problem)\n",
    "    return nums, problems\n",
    "\n",
    "def sort_nums(data):\n",
    "    nums_sorted = []\n",
    "    nums_no_const_sorted = []\n",
    "    for nums in data.nums_no_const:\n",
    "        nums_no_const_sorted.append(sorted(list(eval(nums)), key=lambda x: float(x)))\n",
    "    for nums in data.nums:\n",
    "        num_list = list(eval(nums))\n",
    "        maximum = max_num(num_list)\n",
    "        get_float = lambda x: float(const2val[x])+maximum if x in const2val else float(x)\n",
    "        nums_sorted.append(sorted(num_list, key=get_float))\n",
    "    return nums_sorted, nums_no_const_sorted\n",
    "\n",
    "#sort_nums(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7260430",
   "metadata": {},
   "source": [
    "Here I do some testing to see if the numbers from the equation can be found in the problem description using simple regexes. This actually works extremely well, having no examples where the expected numbers is not a subset of the obtained numbers. This does not include constants. Constants are values which should not occur in the problem description (like pi or the 2 in r^2 for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43603d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = remove_const(data['train'])\n",
    "obtained,_ = get_nums_from_problem(data['train'], convert_to_float=True)\n",
    "obtained = [set(x) for x in obtained]\n",
    "\n",
    "idx = 0\n",
    "for x, y in zip(expected, obtained):\n",
    "    if not (x <= y):\n",
    "        print('------------------')\n",
    "        print(data['train']['problem'][idx])\n",
    "        print(f'Expected: {x}')\n",
    "        print(f'Obtained: {y}')\n",
    "        print('------------------')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe01d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "general        7187\n",
       "physics        4885\n",
       "gain           3520\n",
       "geometry       1410\n",
       "other          1069\n",
       "probability     144\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bc1ac-89a5-4e66-a781-4595950ab9f4",
   "metadata": {},
   "source": [
    "## Masked Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfc7b7-7194-4dcd-8423-99080999cd25",
   "metadata": {},
   "source": [
    "We use Roberta to get contextualized embeddings for each math problem\n",
    "\n",
    "First, the problem texts must be tokenized into input ids. A number mask token is used for each number in the problem, as they should not affect the problem itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61dd08b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at models/distilroberta-base-encoder-mathqa and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = f'{MODEL_DIR}{ENCODER_MODEL}-encoder-mathqa'\n",
    "model = AutoModelForMaskedLM.from_pretrained(ENCODER_MODEL) # Used for fine tuning only\n",
    "#model = AutoModel.from_pretrained(model_path, output_hidden_states=True) # Fine tuned model used for getting the contextualized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd053ea-161e-44a7-bb13-74e4ca6a0c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50266. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ENCODER_MODEL)\n",
    "\n",
    "# Adding a new token to the model, for masking out numbers.\n",
    "tokenizer.add_special_tokens({'additional_special_tokens':[NUM_MASK]})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def tokenize_data(data):\n",
    "    tokenization = lambda x: tokenizer(x, padding='max_length', max_length=MAX_TOKENS, truncation=True)\n",
    "    _,problem = get_nums_from_problem(data)\n",
    "    \n",
    "    tokenized = list(map(tokenization, problem))\n",
    "    input_ids = torch.stack([torch.tensor(x['input_ids']) for x in tokenized])\n",
    "    attention_mask = torch.stack([torch.tensor(x['attention_mask']) for x in tokenized])\n",
    "    \n",
    "    return {'input_ids':input_ids.long(), 'attention_mask':attention_mask.int()}\n",
    "\n",
    "tokenized = {name:tokenize_data(data[name]) for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fabc7e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of problems that exceed 392 tokens: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of problems that exceed {MAX_TOKENS} tokens: {np.sum(np.array((tokenized['train']['input_ids'][:,-1]!=1)))}\") # 1 is the padding token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9a3ee",
   "metadata": {},
   "source": [
    "Next, the encoder model is finetuned on MathQA, using masked language modeling, similar to how bert does its trainined. This allows the model to create better contextualized representations for each math problem. Hyperparameters courtesy of https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb#scrollTo=QRTpmyCc3l_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b105b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f'{WORKING_DIR}{model_path}',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    ")\n",
    "\n",
    "train = Dataset.from_dict(tokenized['train'])\n",
    "val = Dataset.from_dict(tokenized['validation'])\n",
    "train.set_format('torch')\n",
    "val.set_format('torch')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15) # using the masked probability from BERT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c8ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c0a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f960b6cd",
   "metadata": {},
   "source": [
    "This function gets the number index for each masked number token in the tokenized problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf5610d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_idx(tokenized):\n",
    "    mask_id = tokenizer.encode(NUM_MASK, add_special_tokens=False)[0]\n",
    "    ids = tokenized['input_ids']\n",
    "    \n",
    "    return [np.where(id==mask_id)[0] for id in ids]\n",
    "\n",
    "masked_idx = {name:get_masked_idx(tokenized[name]) for name in data.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0cbac",
   "metadata": {},
   "source": [
    "This function gets the problem indices that each constant is used in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6a72e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_const_problems(data):\n",
    "    const2idx = {const:[] for const in Const._value2member_map_.keys()}\n",
    "    for idx, num_list in enumerate(data.nums):\n",
    "        for x in eval(num_list):\n",
    "            if x in const2val:\n",
    "                const2idx[x].append(idx)\n",
    "    return {k:np.array(v) for k,v in const2idx.items()}\n",
    "\n",
    "const2idx = {name:get_const_problems(data[name]) for name in SET_NAMES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4659ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
